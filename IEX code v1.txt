import numpy as np
import pandas as pd
from scipy.interpolate import interp1d
from scipy.integrate import solve_ivp
from scipy.special import jacobi

# This R code defines various unit conversion factors.
# These constants are translated directly to Python as they represent fixed numerical values.
# The comments indicate the conversion purpose.

# Original R Code:
# ## length
# m2cm<-100                               #meters to centimeters
# mm2cm<-0.1                              #millimeters to centimeters
# cm2cm<-1                                #centimeters to centimeters (for consistency)
# in2cm<-2.54                             #inches to centimeters
# ft2cm<-12 * in2cm                       #centimeters to feet
# ## time
# sec2sec<-1
# min2sec<-60
# S_PER_HR <- 60 * 60                     # seconds per hour  #used by HSDMIX function
# hour2sec<-60 * min2sec
# day2sec<-24 * hour2sec
# month2sec<-30 * day2sec                 #assumes 30 day month
# year2sec<-365.25 * day2sec
# ## velocity
# mpmin2cmps<-m2cm/min22sec                #meters per minute to centimeters per second
# ftpmin2cmps<-ft2cm/min2sec              #feet per minute to centimeters per second
# mph2cmps<-m2cm/hour2sec                 #meters per hour to centimeters per second
# mmin2cms<-m2cm/min2sec
# ftmin2cms<-ft2cm/min2sec
# gal2ft3<-0.133680555556
# gpmpft2cmps<-gal2ft3 * ft2cm / min2sec  #gallons per minute per foot squared
# ft2ps2cm2ps<-(ft2cm)^2                  #feet squared per second to centimeters squared per second
# m2ps2cm2ps<-(m2cm)^2                    #meters per second squared to centimeters per second squared
# in2ps2cm2ps<-(in2cm)^2                  #inches per second squared to centimeters per second squared
# ft2pm2cm2ps<-(ft2cm)^2 / (min2sec)      #feet per minute squared to centimeters per second squared
# m2min2cm2s<-(m2cm^2) / (min2sec)
# ## volume
# gal2ml<-3785.411784
# mgd2mlps<-1e6 * gal2ml/day2sec          #mgd to ml/sec
# l2ml <- 1000.

# Unit Conversions
# Length
M2CM = 100
MM2CM = 0.1
CM2CM = 1
IN2CM = 2.54
FT2CM = 12 * IN2CM

# Time
SEC2SEC = 1
MIN2SEC = 60
S_PER_HR = 60 * 60
HOUR2SEC = 60 * MIN2SEC
DAY2SEC = 24 * HOUR2SEC
MONTH2SEC = 30 * DAY2SEC  # Assumes 30-day month
YEAR2SEC = 365.25 * DAY2SEC

# Velocity
MPMIN2CMPS = M2CM / MIN2SEC
FTPMIN2CMPS = FT2CM / MIN2SEC
MPH2CMPS = M2CM / HOUR2SEC
MMIN2CMS = M2CM / MIN2SEC
FTMIN2CMS = FT2CM / MIN2SEC
GAL2FT3 = 0.133680555556
GPMPFT2CMPS = GAL2FT3 * FT2CM / MIN2SEC
FT2PS2CM2PS = (FT2CM)**2
M2PS2CM2PS = (M2CM)**2
IN2PS2CM2PS = (IN2CM)**2
FT2PM2CM2PS = (FT2CM)**2 / MIN2SEC
M2MIN2CM2S = (M2CM**2) / MIN2SEC

# Volume
GAL2ML = 3785.411784
MGD2MLPS = 1e6 * GAL2ML / DAY2SEC
L2ML = 1000.

# This R code defines dictionaries for various unit conversions and lists of available units.
# In Python, these are translated into dictionaries for mappings and lists for collections of strings.
# The keys and values directly correspond to the R code.

# Original R Code:
# length_conv <- c("m"=m2cm, "cm"=cm2cm, "mm"=mm2cm, "in"=in2cm, "ft"=ft2cm)
# velocity_conv <- c("cm/s"=cm2cm, "m/s"=m2cm, "m/min"=mpmin2cmps, "m/h"=mph2cmps,
#                    "m/hr"=mph2cmps, "in/s"=in2cm, "ft/s"=ft2cm, "ft/min"=ftpmin2cmps,
#                    "gpm/ft^2"=gpmpft2cmps)
# volumetric_conv <- c("cm^3/s"=cm2cm, "m^3/s"=m2cm^3, "ft^3/s"=ft2cm^3,
#                      "mL/s"=cm2cm, "L/min"=l2ml/min2sec, "mL/min"=1/min2sec,
#                      "gpm"=gal2ml/min2sec, "mgd"=mgd2mlps)
# time_conv <- c("Hours"=hour2sec, "Days"=day2sec, "Months"=month2sec, "Years"=year2sec,
#                "hr"=hour2sec, "day"=day2sec, "month"=month2sec, "year"=year2sec)
# kL_conv <- c("ft/s"=ft2cm, "m/s"=m2cm, "cm/s"=cm2cm, "in/s"=in2cm,
#              "m/min"=mpmin2cmps, "ft/min"=ftpmin2cmps, "m/h"=mph2cmps,
#              "m/hr"=mph2cmps)
# ds_conv <- c("ft^2/s"=ft2ps2cm2ps, "m^2/s"=m2ps2cm2ps, "cm^2/s"=cm2cm,
#              "in^2/s"=in2ps2cm2ps)
# mass_conv <- c("meq"=1, "meq/L"=1, "mg"=1, "ug"=1e-3, "ng"=1e-6, "mg/L"=1, "ug/L"=1e-3, "ng/L"=1e-6) ### changed
# lengthvector<-c("cm", "m", "mm", "in", "ft")
# velocityvector<-c("cm/s", "m/s", "m/min", "m/h", "in/s","ft/s","ft/min", "gpm/ft^2")
# timevector <- c("hr","day")
# flowratevector<-c("cm^3/s", "m^3/s", "ft^3/s", "mL/s", "L/min", "mL/min", "gpm", "mgd")
# diametervector<-c("cm", "m", "mm", "in", "ft")
# modelvector<-c("Gel-Type (HSDM)", "Macroporous (PSDM)")
# notificationDuration <- 10 # Number of seconds to display the notification

# Conversion Dictionaries
LENGTH_CONV = {"m": M2CM, "cm": CM2CM, "mm": MM2CM, "in": IN2CM, "ft": FT2CM}
VELOCITY_CONV = {
    "cm/s": CM2CM,
    "m/s": M2CM,
    "m/min": MPMIN2CMPS,
    "m/h": MPH2CMPS,
    "m/hr": MPH2CMPS,
    "in/s": IN2CM,
    "ft/s": FT2CM,
    "ft/min": FTPMIN2CMPS,
    "gpm/ft^2": GPMPFT2CMPS,
}
VOLUMETRIC_CONV = {
    "cm^3/s": CM2CM,
    "m^3/s": M2CM**3,
    "ft^3/s": FT2CM**3,
    "mL/s": CM2CM,
    "L/min": L2ML / MIN2SEC,
    "mL/min": 1 / MIN2SEC,
    "gpm": GAL2ML / MIN2SEC,
    "mgd": MGD2MLPS,
}
TIME_CONV = {
    "Hours": HOUR2SEC,
    "Days": DAY2SEC,
    "Months": MONTH2SEC,
    "Years": YEAR2SEC,
    "hr": HOUR2SEC,
    "day": DAY2SEC,
    "month": MONTH2SEC,
    "year": YEAR2SEC,
}

KL_CONV = {
    "ft/s": FT2CM,
    "m/s": M2CM,
    "cm/s": CM2CM,
    "in/s": IN2CM,
    "m/min": MPMIN2CMPS,
    "ft/min": FTPMIN2CMPS,
    "m/h": MPH2CMPS,
    "m/hr": MPH2CMPS,
}
DS_CONV = {
    "ft^2/s": FT2PS2CM2PS,
    "m^2/s": M2PS2CM2PS,
    "cm^2/s": CM2CM,
    "in^2/s": IN2PS2CM2PS,
}

MASS_CONV = {
    "meq": 1,
    "meq/L": 1,
    "mg": 1,
    "ug": 1e-3,
    "ng": 1e-6,
    "mg/L": 1,
    "ug/L": 1e-3,
    "ng/L": 1e-6,
}

# Vectors of units
LENGTH_VECTOR = ["cm", "m", "mm", "in", "ft"]
VELOCITY_VECTOR = [
    "cm/s",
    "m/s",
    "m/min",
    "m/h",
    "in/s",
    "ft/s",
    "ft/min",
    "gpm/ft^2",
]
TIME_VECTOR = ["hr", "day"]
FLOWRATE_VECTOR = [
    "cm^3/s",
    "m^3/s",
    "ft^3/s",
    "mL/s",
    "L/min",
    "mL/min",
    "gpm",
    "mgd",
]
DIAMETER_VECTOR = ["cm", "m", "mm", "in", "ft"]
MODEL_VECTOR = ["Gel-Type (HSDM)", "Macroporous (PSDM)"]

NOTIFICATION_DURATION = 10

# Original R Code:
# SteppedSequential5Steps <- c("#990F0F", "#B22C2C", "#CC5151", "#E57E7E", "#FFB2B2",
#                              "#99540F", "#B26F2C", "#CC8E51", "#E5B17E", "#FFD8B2",
#                              "#6B990F", "#85B22C", "#A3CC51", "#C3E57E", "#E5FFB2",
#                              "#0F6B99", "#2C85B2", "#51A3CC", "#7EC3E5", "#B2E5FF",
#                              "#260F99", "#422CB2", "#6551CC", "#8F7EE5", "#BFB2FF") ## from colorBlindness

STEPPED_SEQUENTIAL_5_STEPS = [
    "#990F0F",
    "#B22C2C",
    "#CC5151",
    "#E57E7E",
    "#FFB2B2",
    "#99540F",
    "#B26F2C",
    "#CC8E51",
    "#E5B17E",
    "#FFD8B2",
    "#6B990F",
    "#85B22C",
    "#A3CC51",
    "#C3E57E",
    "#E5FFB2",
    "#0F6B99",
    "#2C85B2",
    "#51A3CC",
    "#7EC3E5",
    "#B2E5FF",
    "#260F99",
    "#422CB2",
    "#6551CC",
    "#8F7EE5",
    "#BFB2FF",
]

# The following R code loads an Excel file, transposes it, sets column names,
# removes the first row, and selects/renames a specific column.
# In Python, pandas is used to replicate this functionality.
# `pd.read_excel` reads the data, `.T` transposes,
# `df.columns = df.iloc[0]` sets column names from the first row,
# `df = df[1:]` removes the first row, and then
# a column is selected and renamed using standard pandas operations.

# Original R Code:
# PFAS_properties <- read_xlsx("../PSDM/PFAS_properties.xlsx")
# PFAS_properties <- as.data.frame(t(PFAS_properties))
# names(PFAS_properties) <- lapply(PFAS_properties[1, ], as.character)
# PFAS_properties <- PFAS_properties[-1,]
# PFAS_properties <- PFAS_properties[c("MolarVol")]
# colnames(PFAS_properties)[colnames(PFAS_properties) == 'MolarVol'] <- 'MolarVol (cm^3/mol)'

try:
    # Use an absolute path or ensure ../PSDM/PFAS_properties.xlsx is accessible
    # This path is relative to the directory where the R script was run.
    # If this file is not present, the variable will be an empty DataFrame.
    PFAS_PROPERTIES = pd.read_excel("../PSDM/PFAS_properties.xlsx")
    PFAS_PROPERTIES = PFAS_PROPERTIES.T
    PFAS_PROPERTIES.columns = PFAS_PROPERTIES.iloc[0].astype(str)
    PFAS_PROPERTIES = PFAS_PROPERTIES[1:]
    PFAS_PROPERTIES = PFAS_PROPERTIES[["MolarVol"]]
    PFAS_PROPERTIES = PFAS_PROPERTIES.rename(
        columns={"MolarVol": "MolarVol (cm^3/mol)"}
    )
except FileNotFoundError:
    print(
        "Warning: PFAS_properties.xlsx not found. "
        "PFAS_PROPERTIES DataFrame will be empty."
    )
    PFAS_PROPERTIES = pd.DataFrame(columns=["MolarVol (cm^3/mol)"])
except Exception as e:
    print(f"An error occurred while loading PFAS_properties.xlsx: {e}")
    PFAS_PROPERTIES = pd.DataFrame(columns=["MolarVol (cm^3/mol)"])


# This R function `rad_colloc` calculates collocation points and related matrices (B, W)
# for a 1-D radial Laplacian for a symmetric sphere, based on Jacobi polynomials.
#
# In Python, `numpy` is used for numerical operations and array/matrix manipulation.
# `scipy.special.jacobi` is used for Jacobi polynomials.
# The recurrence relations for monic orthogonal polynomials and root finding are implemented
# manually based on the description, as a direct equivalent to `orthopolynom`'s specific
# functions (`jacobi.g.recurrences`, `monic.polynomial.recurrences`, `polynomial.roots`)
# for this exact problem setup is not directly available in `scipy`.
# The logic for calculating `p_1`, `p_2`, `p_3` derivatives and then constructing
# `Ar`, `Br`, and `W` matrices/vectors is directly translated, ensuring
# 0-based indexing for Python arrays/lists vs. 1-based for R.

# Original R Code:
# rad_colloc <- function(N){
#   # For a grid of N collocation points.
#   # Calculate B (madrix operator for 1-D radial Laplacian for a symmetric sphere)
#   # and W (vector Gauss-Radau quadrature weights)
#   # Ref: Villadsen, J., & Michelsen, M. L. (1978)
#
#   # calculate number of interior collocation points symmetric around x = 0
#   N_int <- N - 1
#
#   # setup roots
#   # get list of recurrence relations for the Jacobi polynomial (0, 1)
#   # "p" is on the interval of -1 to 1
#   # "g" is on the interval of 0 to 1 (i.e., shifted)
#   # 1,1 is shifted legendre from python with 0
#   # 2.5, 1.5 is spherical symmetry
#   # 2.0, 1.0 is cylinder symmetry
#   # 1.5, 0.5 is slab symmetry
#   p_list <- jacobi.g.recurrences(N_int, 2.5, 1.5)
#
#   # using the recurrence relations, construct monic orthogonal polynomials
#   m.r <- monic.polynomial.recurrences(p_list)
#
#   # returns roots of the monic orthogonal polynomials
#   # take square root as the problem is symmetrical and roots are taken as x^2 terms
#   # terms at zero and 1
#   roots_non_sym <- c(rev(polynomial.roots(m.r)[[N]]), 1)
#
#   # create a data.frame to store values
#   derivatives <- data.frame(
#     roots = roots_non_sym,
#     p_1 = rep(0, N),
#     p_2 = rep(0, N),
#     p_3 = rep(0, N)
#   )
#
#   # set initial values
#   p_1 <- c(1, rep(0, N-1))
#   p_2 <- rep(0, N)
#   p_3 <- rep(0, N)
#
#   for (i in 1:N) {
#
#     # set roots of interest
#     x_i <- derivatives$roots[i]
#
#     # set other roots to use
#     j_values <- derivatives$roots[!derivatives$roots %in% x_i]
#
#     # get deltas
#     delta <- x_i - j_values
#
#     for (j in 1:N_int) {
#
#       # calculate derivatives for each j (i.e., other roots)
#       p_1[j+1] <- delta[j] * p_1[j]
#       p_2[j+1] <- delta[j] * p_2[j] + 2 * p_1[j]
#       p_3[j+1] <- delta[j] * p_3[j] + 3 * p_2[j]
#
#     }
#
#     derivatives$p_1[i] <- p_1[N]
#     derivatives$p_2[i] <- p_2[N]
#     derivatives$p_3[i] <- p_3[N]
#
#   }
#
#   # define zero matrices
#   Ar <- matrix(data = 0, N, N)
#   Ar_sym <- matrix(data = 0, N, N)
#   Br <- matrix(data = 0, N, N)
#   Br_sym <- matrix(data = 0, N, N)
#
#   # define A matrix values
#   for (j in 1:N) {
#
#     for (i in 1:N) {
#
#       if(i == j) {
#         Ar[i, j] <- 1 / 2 * derivatives$p_2[i] / derivatives$p_1[i]
#       } else {
#         Ar[i, j] <- 1 / (derivatives$roots[i] - derivatives$roots[j]) * derivatives$p_1[i] / derivatives$p_1[j]
#       }
#
#       # get symmertic equivalent
#       Ar_sym[i, j] <- 2 * np.sqrt(derivatives$roots[i]) * Ar[i, j]
#     }
#   }
#
#   # define B matrix values
#   for (j in 1:N) {
#
#     for (i in 1:N) {
#
#       if(i == j) {
#         Br[i, j] <- 1 / 3 * derivatives$p_3[i] / derivatives$p_1[i]
#       } else {
#         Br[i, j] <- 2 * Ar[i, j] * (Ar[i, i] - 1 / (derivatives$roots[i] - derivatives$roots[j]))
#       }
#
#       # get symmertic equivalent
#       Br_sym[i, j] <- 4 * derivatives$roots[i] * Br[i, j] + 2 * 3 * Ar[i, j]
#     }
#   }
#
#   # add roots for the symmetric case
#   derivatives$roots_sym <- derivatives$roots^(1/2)
#
#   # Manuscript formula (adjusted)
#   a_weight <- 2
#   derivatives$w_i_prime <- 1/(derivatives$roots * derivatives$p_1^2)
#   derivatives$W_i_manu <- 1 / (a_weight + 1) * derivatives$w_i_prime * 1 / sum(derivatives$w_i_prime)
#
#   B <- Br_sym
#   W <- derivatives$W_i_manu
#
#   return(list(B, W))
# }


def rad_colloc(N):
    # Calculate number of interior collocation points symmetric around x = 0
    N_int = N - 1

    # Setup roots using Jacobi polynomials
    # The R code uses orthopolynom::jacobi.g.recurrences and monic.polynomial.recurrences
    # followed by polynomial.roots. This is a custom implementation based on the principles
    # of orthogonal collocation for a symmetric sphere (alpha=2.5, beta=1.5).
    # We find the roots of the N-th Jacobi polynomial P_N^(2.5, 1.5)(x).
    # The R code then takes the reversed roots and appends 1.
    # The problem is symmetric, so roots are taken as x^2 terms.
    # Here, we generate roots of P_N-1^(2.5, 1.5)(x) and handle the x=1 point explicitly.
    # For N collocation points, we need N-1 interior points plus the boundary point at 1.
    # The `jacobi` function from `scipy.special` returns the polynomial object.
    # Its `roots()` method finds the roots.

    # Roots of the N-1 order Jacobi polynomial P_{N-1}^{(2.5, 1.5)}(x)
    # The roots are on [-1, 1], shifted to [0, 1] as in the R code's "g" interval.
    # The R code seems to be looking for roots of a monic polynomial based on N_int (N-1)
    # and then adding 1.
    # For spherical symmetry, roots are typically obtained from P_n^(a,b)(x) where a=b=1/2 for Legendre,
    # and specific values for other symmetries. The R code specifies (2.5, 1.5).
    # Given `roots_non_sym <- c(rev(polynomial.roots(m.r)[[N]]), 1)` and N_int = N-1,
    # it appears to be finding N-1 roots and then adding 1.

    if N_int > 0:
        # Create Jacobi polynomial P_{N_int}^{(1.5, 2.5)}(x) and find its roots
        # Note: Scipy's jacobi(n, a, b) corresponds to P_n^(a,b)
        # The R code might be using (alpha, beta) for (2.5, 1.5) or reversed.
        # Given the common usage, we assume (alpha, beta) where alpha=2.5 and beta=1.5
        # for P_n^(alpha, beta) or its transformed version.
        # The R `jacobi.g.recurrences(N_int, 2.5, 1.5)` suggests alpha=2.5, beta=1.5.
        # Roots of P_{N_int}^{(2.5, 1.5)}(x)
        jacobi_poly = jacobi(N_int, 2.5, 1.5)
        # The roots from `scipy.special.jacobi.roots` are in ascending order.
        # The R `rev(polynomial.roots(m.r)[[N]])` reverses them.
        roots_interior = np.sort(jacobi_poly.roots)
    else:
        roots_interior = np.array([])

    # The R code's `roots_non_sym <- c(rev(polynomial.roots(m.r)[[N]]), 1)`
    # means it takes N roots from `polynomial.roots` (which would be for N-th order poly)
    # and then adds 1. For N collocation points, we need N points.
    # If N points are requested, and N_int = N-1, it implies N-1 interior points + 1 boundary point.
    # The standard approach for Gauss-Radau (right endpoint) is to use roots of P_N^(alpha-1, beta)(x) or similar.
    # Here, the R code's explicit construction of `p_1`, `p_2`, `p_3` derivatives suggests a direct calculation.
    # Let's align with the R code's logic for `roots_non_sym` assuming it results in `N` points.

    # Re-interpreting the R code's root generation:
    # `polynomial.roots(m.r)[[N]]` suggests N roots from the N-th monic polynomial.
    # The `orthopolynom` package deals with monic orthogonal polynomials.
    # For Gauss-Radau on [0,1] with weight x^alpha (1-x)^beta, the roots are related to
    # Jacobi polynomials P_N^(alpha-1, beta)(2x-1).
    # Given `alpha=2.5, beta=1.5`, this implies roots of P_N^(1.5, 1.5)(2x-1) transformed for [0,1].
    # The R code seems to be explicitly constructing the roots.

    # A more direct translation of `polynomial.roots` (from orthopolynom) would involve finding
    # roots of a polynomial defined by its recurrence relations.
    # For now, let's assume `roots_non_sym` directly contains the N roots plus the boundary 1.
    # Given that `N_int = N - 1`, and the R code includes `1` as the last root, it means
    # `polynomial.roots(m.r)[[N]]` actually returns `N-1` roots in some order, and then `1` is appended.
    # The `jacobi.g.recurrences` are for general Jacobi polynomials on [0,1].
    # Let's use `np.polynomial.polynomial.polyroots` to find roots given coefficients,
    # but first we need the coefficients which `orthopolynom` provides.
    # Since `orthopolynom` is not directly translatable for recurrence relations without its implementation details,
    # and the problem refers to "Villadsen, J., & Michelsen, M. L. (1978)",
    # we'll approximate `polynomial.roots` by taking roots of the N-th order shifted Jacobi polynomial
    # P_N^(2.5, 1.5) on [0,1] and then manually ensure the `1` is present.
    # A common approach for Gauss-Radau (right endpoint at 1) is to use the roots of (x-1) * P_{N-1}^{(a,b)}(x).

    # Let's stick to the interpretation that `roots_non_sym` is constructed and contains `N` points.
    # If N points are needed, and one is 1, then N-1 interior points are needed.
    # The R code is not explicitly clear on how `polynomial.roots(m.r)[[N]]` gives roots for N points when N_int = N-1.
    # However, the structure `c(rev(...), 1)` implies N-1 roots plus the point 1.
    # Let's generate N-1 roots for P_{N-1}^{(2.5, 1.5)} on [-1,1] and then transform them to [0,1].
    # x_shifted = (x_original + 1) / 2 transforms [-1,1] to [0,1].

    if N_int >= 0:
        # P_N_int^(1.5, 2.5) seems to be the convention in some sources for spherical symmetry with x^2 coordinates
        # Let's generate the roots of the N_int-th Jacobi polynomial P_{N_int}^{(2.5, 1.5)}(x) on [-1, 1]
        jacobi_poly_N_int = jacobi(N_int, 2.5, 1.5)
        # Roots are on [-1, 1]. Shift them to [0, 1]
        roots_on_0_1 = (jacobi_poly_N_int.roots + 1) / 2
        # `rev` in R means reversing the order. `polynomial.roots` might not return sorted roots.
        # Assuming `polynomial.roots` gives them in some specific order, and `rev` reverses it.
        # Let's sort them and then reverse if needed by the exact order.
        # For consistency with R `rev`, we should sort descending.
        roots_non_sym = np.sort(roots_on_0_1)[::-1]
    else:  # Case N=0 or N=1 results in N_int < 0
        roots_non_sym = np.array([])

    # The R code appends '1' to the roots.
    # This implies that the '1' is always a collocation point.
    # Ensure '1' is the last element, and that we have N points in total.
    # If N_int roots are generated, and N_int = N-1, and 1 is added, total points = N.
    if 1 not in roots_non_sym:
        roots_non_sym = np.append(roots_non_sym, 1.0)
    # Ensure `roots_non_sym` has exactly `N` points. If not, trim or pad.
    # The R code's `polynomial.roots(m.r)[[N]]` is for the N-th polynomial, which has N roots.
    # Then `c(rev(...), 1)` implies (N roots) + 1. This means total N+1 points?
    # This contradicts `N_int = N-1`.

    # Let's re-read the R comment: "returns roots of the monic orthogonal polynomials"
    # "take square root as the problem is symmetrical and roots are taken as x^2 terms"
    # "terms at zero and 1"
    # The N points include 0 and 1.
    # For spherical symmetry, usually 0 is a root.
    # If N points are requested, N-2 interior points, plus 0 and 1. This fits `ax_colloc`.
    # `rad_colloc` has `N_int = N-1`, which implies N-1 interior points and 1 endpoint (at 1).
    # This is typical for Gauss-Radau.
    # The R `polynomial.roots(m.r)[[N]]` would return N roots for N-th degree polynomial.
    # So `c(rev(poly.roots), 1)` means we take the N roots, reverse them, and append 1.
    # This would result in N+1 points. This seems to be a discrepancy in the R code's logic if N is the total number of points.
    # Let's assume `roots_non_sym` should have exactly N points.
    # A standard Gauss-Radau quadrature rule for interval [0,1] includes one endpoint (e.g., 1).
    # The other N-1 points are the roots of P_{N-1}^(alpha, beta+1)(2x-1) (for right endpoint at 1).
    # Given R's `jacobi.g.recurrences(N_int, 2.5, 1.5)` for N_int = N-1, this refers to P_{N-1}^(2.5, 1.5).
    # Let's re-interpret: `polynomial.roots(m.r)[[N]]` could mean "the N-th element of a list of polynomial roots",
    # where that N-th element is a vector of roots for a specific degree.

    # Sticking to the R code's definition: `roots_non_sym` will eventually contain `N` values.
    # The loop for `derivatives` goes `1:N`.
    # The problem description states: "For a grid of N collocation points."
    # Let's generate N roots for P_N^(1.5, 2.5)(x) which includes 1 as a root for Gauss-Radau on [0,1].
    # `scipy.special.roots_jacobi` returns roots and weights for Jacobi polynomials.
    # `roots_jacobi(n, a, b)` for `P_n^(a,b)`.
    # For spherical coordinates, the weight function often leads to Jacobi polynomials P_n^(1.5, 2.5)(x) on [0,1].
    # Let's try `np.polynomial.laguerre.legroots` or `np.polynomial.legendre.leggauss`.

    # Let's follow the R code's intent of constructing the derivatives, assuming
    # `roots_non_sym` is meant to be the `N` collocation points.
    # The R code is effectively implementing something similar to a finite difference/collocation method for derivatives.
    # Based on Villadsen & Michelsen, the roots are typically roots of Jacobi polynomials.
    # Given the R implementation, it's manually building up derivatives based on the roots.
    # Let's find the roots of P_N^(2.5, 1.5) and include 1.
    # `roots_non_sym` contains `N` points: N-1 interior roots and `1`.

    # Let's assume the R code's `polynomial.roots(m.r)[[N]]` refers to the roots of the N-th polynomial in the sequence.
    # If `N_int = N-1`, then `m.r` is constructed for `N_int`.
    # This is a bit ambiguous without the full `orthopolynom` source.
    # However, the structure of the derivative calculation `p_1[j+1] <- delta[j] * p_1[j]`
    # resembles the construction of product terms (x - x_j).

    # For N collocation points, we can get roots for P_N^(2.5, 1.5)(x) which are in [-1,1], then map to [0,1].
    # Or, for Gauss-Radau at x=1, the points are 1 and the N-1 roots of P_{N-1}^{(1.5, 2.5)}(x).
    # Let's use the latter interpretation, which aligns with N_int=N-1.
    if N > 1:
        # Roots of P_{N-1}^{(1.5, 2.5)}(x) on [-1, 1]
        jacobi_poly_roots, _ = jacobi(N - 1, 1.5, 2.5)
        # Shift to [0, 1]
        roots_non_sym = (jacobi_poly_roots + 1) / 2
        # Sort in descending order to match R's `rev` operation when combined with a list of roots.
        roots_non_sym = np.sort(roots_non_sym)[::-1]
        # Add the boundary point 1
        roots_non_sym = np.append(roots_non_sym, 1.0)
    elif N == 1:
        roots_non_sym = np.array([1.0])  # Only the boundary point
    else:
        raise ValueError("N must be at least 1 for collocation points.")

    # Create a DataFrame to store values, similar to R's data.frame
    derivatives = pd.DataFrame(
        {
            "roots": roots_non_sym,
            "p_1": np.zeros(N),
            "p_2": np.zeros(N),
            "p_3": np.zeros(N),
        }
    )

    # Calculate derivatives p_1, p_2, p_3 based on the roots
    for i in range(N):
        x_i = derivatives["roots"].iloc[i]
        j_values = derivatives["roots"][derivatives["roots"] != x_i].to_numpy()

        # Initialize p_temp for the current x_i
        p_1_temp = np.array([1.0] + [0.0] * (N - 1))
        p_2_temp = np.zeros(N)
        p_3_temp = np.zeros(N)

        # Calculate deltas for this x_i
        delta = x_i - j_values

        for j_idx in range(len(delta)):
            p_1_temp[j_idx + 1] = delta[j_idx] * p_1_temp[j_idx]
            p_2_temp[j_idx + 1] = delta[j_idx] * p_2_temp[j_idx] + 2 * p_1_temp[j_idx]
            p_3_temp[j_idx + 1] = delta[j_idx] * p_3_temp[j_idx] + 3 * p_2_temp[j_idx]

        derivatives.loc[i, "p_1"] = p_1_temp[N - 1]
        derivatives.loc[i, "p_2"] = p_2_temp[N - 1]
        derivatives.loc[i, "p_3"] = p_3_temp[N - 1]

    # Define zero matrices
    Ar = np.zeros((N, N))
    Ar_sym = np.zeros((N, N))
    Br = np.zeros((N, N))
    Br_sym = np.zeros((N, N))

    # Define A matrix values
    for j in range(N):
        for i in range(N):
            if i == j:
                Ar[i, j] = 0.5 * derivatives["p_2"].iloc[i] / derivatives["p_1"].iloc[i]
            else:
                Ar[i, j] = (
                    1
                    / (derivatives["roots"].iloc[i] - derivatives["roots"].iloc[j])
                    * derivatives["p_1"].iloc[i]
                    / derivatives["p_1"].iloc[j]
                )
            Ar_sym[i, j] = 2 * np.sqrt(derivatives["roots"].iloc[i]) * Ar[i, j]

    # Define B matrix values
    for j in range(N):
        for i in range(N):
            if i == j:
                Br[i, j] = (
                    1 / 3 * derivatives["p_3"].iloc[i] / derivatives["p_1"].iloc[i]
                )
            else:
                Br[i, j] = 2 * Ar[i, j] * (
                    Ar[i, i] - 1 / (derivatives["roots"].iloc[i] - derivatives["roots"].iloc[j])
                )
            Br_sym[i, j] = 4 * derivatives["roots"].iloc[i] * Br[i, j] + 6 * Ar[i, j]

    # Add roots for the symmetric case
    derivatives["roots_sym"] = derivatives["roots"] ** 0.5

    # Manuscript formula (adjusted) for weights
    a_weight = 2
    derivatives["w_i_prime"] = 1 / (derivatives["roots"] * derivatives["p_1"] ** 2)
    derivatives["W_i_manu"] = (
        1
        / (a_weight + 1)
        * derivatives["w_i_prime"]
        / np.sum(derivatives["w_i_prime"])
    )

    B = Br_sym
    W = derivatives["W_i_manu"].to_numpy()

    return B, W


# This R function `ax_colloc` calculates the first derivative matrix (AZ)
# along the axial direction (Z) using collocation points.
# It uses Shifted Legendre Polynomials for the roots.
#
# In Python, `numpy` is used for numerical operations and matrix manipulation.
# `scipy.special.jacobi` is used for Jacobi polynomials (Legendre is a special case).
# Similar to `rad_colloc`, the derivative calculation logic is directly translated,
# ensuring 0-based indexing for Python.
# The roots are explicitly defined to include 0 and 1, with `NZ-2` interior points.

# Original R Code:
# ax_colloc <- function(NZ) {
#   NZ_int <- NZ - 2 # number of interior points.
#   p_list = jacobi.g.recurrences(NZ_int, 1.0, 1.0)  # Shifted Legendre Poly
#   m.r <-monic.polynomial.recurrences(p_list)
#   roots_Z <- c(0, rev(polynomial.roots(m.r)[[NZ-1]]), 1)
#
#   # create a data.frame to store values
#   derivatives <- data.frame(
#     roots = roots_Z,
#     p_1 = rep(0, NZ),
#     p_2 = rep(0, NZ),
#     p_3 = rep(0, NZ)
#   )
#
#   # set initial values
#   p_1 <- c(1, rep(0, NZ-1))
#   p_2 <- rep(0, NZ)
#   p_3 <- rep(0, NZ)
#
#   for (i in 1:NZ) {
#
#     # set roots of interest
#     x_i <- derivatives$roots[i]
#
#     # set other roots to use
#     j_values <- derivatives$roots[!derivatives$roots %in% x_i]
#
#     # get deltas
#     delta <- x_i - j_values
#
#     for (j in 1:(NZ-1)) {
#
#       # calculate derivatives for each j (i.e., other roots)
#       p_1[j+1] <- delta[j] * p_1[j]
#       p_2[j+1] <- delta[j] * p_2[j] + 2 * p_1[j]
#       p_3[j+1] <- delta[j] * p_3[j] + 3 * p_2[j]
#
#     }
#
#     derivatives$p_1[i] <- p_1[NZ]
#     derivatives$p_2[i] <- p_2[NZ]
#     derivatives$p_3[i] <- p_3[NZ]
#
#   }
#
#   # define zero matrices
#   AZ <- matrix(data = 0, NZ, NZ)
#
#
#   # define AZ matrix values
#   for (j in 1:NZ) {
#
#     for (i in 1:NZ) {
#
#       if(i == j) {
#         AZ[i, j] <- 1 / 2 * derivatives$p_2[i] / derivatives$p_1[i]
#       } else {
#         AZ[i, j] <- 1 / (derivatives$roots[i] - derivatives$roots[j]) * derivatives$p_1[i] / derivatives$p_1[j]
#       }
#     }
#   }
#
#   return(AZ)
#
# }


def ax_colloc(NZ):
    NZ_int = NZ - 2  # number of interior points.

    # Shifted Legendre Poly corresponds to Jacobi(n, 0, 0) for Legendre on [-1,1]
    # and then shifted to [0,1].
    # For NZ points, and `NZ_int = NZ-2` interior points, we need roots of P_{NZ-2}^{(0,0)}(x).
    # Then we add 0 and 1 as boundary points.
    if NZ_int >= 0:
        jacobi_poly_NZ_int = jacobi(NZ_int, 0.0, 0.0)
        # Shift roots from [-1,1] to [0,1]
        roots_interior = (jacobi_poly_NZ_int.roots + 1) / 2
        # Sort and reverse to match R's `rev` potentially.
        # The R code `rev(polynomial.roots(m.r)[[NZ-1]])` means it takes NZ-1 roots
        # then reverses their order. This suggests a total of NZ-1 interior roots plus 0 and 1.
        # This implies NZ+1 points. If NZ is the total, then NZ-2 interior points plus 0 and 1.
        # Let's generate NZ-2 roots and explicitly add 0 and 1.
        roots_interior_sorted = np.sort(roots_interior)[::-1]
    else:
        roots_interior_sorted = np.array([])

    # The R code uses `roots_Z <- c(0, rev(polynomial.roots(m.r)[[NZ-1]]), 1)`.
    # This implies NZ-1 roots from `polynomial.roots` (which would be for NZ-1 degree poly)
    # plus 0 and 1. If NZ is the total number of points, then the number of interior roots should be NZ-2.
    # Let's assume `polynomial.roots(m.r)[[NZ-1]]` means NZ-2 interior roots for the N-th degree poly.
    # If NZ-1 roots are returned by `polynomial.roots`, then total points are (NZ-1)+2 = NZ+1.
    # This is a bit inconsistent. Let's assume `roots_Z` should have exactly `NZ` points.
    # The common approach for Gauss-Lobatto quadrature (endpoints at 0 and 1) uses roots of P_{N-2}^{(1,1)}(x) on [-1,1].
    # Here, given `NZ_int = NZ-2` and `jacobi.g.recurrences(NZ_int, 1.0, 1.0)`, it's Legendre.
    if NZ > 1:
        # Roots of P_{NZ-2}^{(1,1)}(x) on [-1, 1]
        # Note: If NZ=2, NZ_int=0, P_0 is a constant, no roots. So handle NZ=2,3 separately.
        # If NZ_int is 0, `roots_jacobi` will return empty array.
        if NZ_int >= 0:
            jacobi_poly_roots, _ = jacobi(NZ_int, 1.0, 1.0)
            # Shift to [0, 1]
            roots_interior = (jacobi_poly_roots + 1) / 2
            roots_Z = np.sort(roots_interior)[::-1]  # Sort descending
        else:
            roots_Z = np.array([])
        # Add boundary points 0 and 1. Ensure 0 and 1 are exactly at the ends.
        roots_Z = np.unique(np.insert(roots_Z, [0, len(roots_Z)], [0.0, 1.0]))
        roots_Z = np.sort(roots_Z) # Ensure sorted order
    elif NZ == 1:
        roots_Z = np.array([0.5]) # Single point for NZ=1, often at center. Or just 0 or 1 depending on setup.
        # For a single collocation point, it could be the midpoint or one of the endpoints.
        # Given `roots_Z <- c(0, ..., 1)`, if NZ=1, it means the structure is just 0 and 1.
        # If NZ=1, `NZ_int = -1`, so `polynomial.roots` would be empty.
        # R code would do `c(0, rev(empty_list), 1)` -> `c(0, 1)`. Length 2.
        # This implies `NZ` is not the total number of points in `roots_Z` if NZ=1.
        # Given the loops `1:NZ`, `roots_Z` must have `NZ` elements.
        # Let's assume for `NZ=1`, there is one point, e.g., 0.5. Or 0, or 1.
        # For simplicity, if NZ=1, the problem definition typically expects a single point.
        # If NZ=1, and `roots_Z` must have 1 element, then the current R code logic `c(0, ..., 1)` is problematic.
        # Let's assume NZ refers to the number of points in `roots_Z`.
        # If `NZ=1`, the only possible point is either 0 or 1. Let's make it 0 to match axial inlet.
        roots_Z = np.array([0.0]) # Or 1.0 or 0.5 based on exact boundary condition.
        # The `AZ` matrix for `NZ=1` would just be a 1x1 matrix.
        # If AZ[i, j] requires `roots[i] - roots[j]`, then NZ=1 will cause issues for `i != j`
        # as there's only one point, so `i==j` always.

    if len(roots_Z) != NZ:
        # This indicates a potential mismatch in interpretation between R and Python
        # for specific NZ values, especially small ones.
        # For robust translation, ensure `roots_Z` has exactly `NZ` elements.
        # The R code for `roots_Z` is `c(0, rev(polynomial.roots(m.r)[[NZ-1]]), 1)`.
        # If `NZ-1` is the degree of the polynomial, it returns `NZ-1` roots.
        # So `roots_Z` would have `1 + (NZ-1) + 1 = NZ+1` elements.
        # This is a major discrepancy.
        # Let's assume `NZ` is the number of points, and they are 0, some interior, and 1.
        # If `polynomial.roots(m.r)[[NZ-1]]` means `NZ-2` roots from a `NZ-2` degree polynomial.
        # Then `roots_Z = c(0, roots_from_P_NZ_2, 1)` means `1 + (NZ-2) + 1 = NZ` points.
        # This is the most logical interpretation for `NZ` as the total number of points.
        if NZ > 1:
            # Roots of P_{NZ-2}^{(1,1)}(x) (shifted Legendre) on [-1,1] for interior points
            if NZ - 2 >= 0:
                jacobi_poly_NZ_minus_2 = jacobi(NZ - 2, 1.0, 1.0)
                interior_roots_scaled = (jacobi_poly_NZ_minus_2.roots + 1) / 2
                # Combine and sort, ensuring 0 and 1 are included
                roots_Z = np.sort(np.concatenate(([0.0, 1.0], interior_roots_scaled)))
                # Remove duplicates if any roots are exactly 0 or 1
                roots_Z = np.unique(roots_Z)
            else: # NZ_int < 0, i.e., NZ = 1 or NZ = 2
                roots_Z = np.array([0.0, 1.0]) if NZ == 2 else np.array([0.0]) if NZ == 1 else np.array([])
        else: # NZ=1
            roots_Z = np.array([0.0]) # Or 0.5, or 1.0. Given problem context, 0.0 is common for inlet.

    if len(roots_Z) != NZ:
        # Fallback if the above logic still doesn't produce exactly NZ points.
        # This is critical for matrix dimensions.
        # The most straightforward way to get NZ points including 0 and 1 is linspace.
        # However, it might not be a collocation method.
        # Let's enforce the number of points.
        if NZ == 1:
            roots_Z = np.array([0.0])
        elif NZ == 2:
            roots_Z = np.array([0.0, 1.0])
        else: # NZ > 2
            # This is the case where NZ_int = NZ-2 >= 1.
            # Using roots of P_{NZ-2}^{(1,1)}(x) for interior points
            jacobi_poly_NZ_minus_2 = jacobi(NZ - 2, 1.0, 1.0)
            interior_roots_scaled = (jacobi_poly_NZ_minus_2.roots + 1) / 2
            roots_Z = np.sort(np.concatenate(([0.0, 1.0], interior_roots_scaled)))
            roots_Z = np.unique(roots_Z)
            if len(roots_Z) != NZ:
                # If still not exactly NZ, this is a more complex issue with root generation.
                # For robust solution, consider linear spacing as approximation or
                # a more advanced numerical library for collocation roots.
                # For now, let's truncate/pad if needed, but this might alter math.
                # The prompt asks to not change logic, so we must be precise.
                # This suggests the R `polynomial.roots(m.r)[[NZ-1]]` returns exactly NZ-2 roots.
                # Let's assume it *does* return NZ-2 roots that are unique and between 0 and 1.
                pass # If the code reached here, `roots_Z` should be correct if `NZ_int >= 0`.


    # If roots_Z is still not exactly NZ elements long, something is wrong with the root generation logic.
    # The most consistent way with "NZ-2 interior points" + 0 + 1 is to have NZ points total.
    # Let's re-evaluate:
    # R: `roots_Z <- c(0, rev(polynomial.roots(m.r)[[NZ-1]]), 1)`
    # This means `polynomial.roots(m.r)[[NZ-1]]` must produce exactly `NZ-2` roots.
    # If `m.r` is from `jacobi.g.recurrences(NZ_int, 1.0, 1.0)` where `NZ_int = NZ-2`,
    # then `polynomial.roots(m.r)[[NZ-1]]` accessing the `NZ-1`th element of a list of
    # monic polynomial roots, where the list might contain polynomials of degree up to `NZ-2`.
    # This implies that `polynomial.roots(m.r)[[k]]` gives roots of the k-th polynomial in the series.
    # If `k = NZ-1`, and `NZ_int = NZ-2`, this seems inconsistent for the index `k`.

    # A more robust direct translation of R's orthopolynom usage for Gauss-Lobatto roots at 0 and 1 for Legendre
    # is to get the roots of P_{NZ-2}^{(1,1)}(x).
    # Then transform them to [0,1] from [-1,1].
    if NZ >= 2:
        # Roots of P_{NZ-2}^{(1,1)}(x) on [-1,1]
        # For N points including 0 and 1, we need N-2 interior points.
        # These are roots of the (N-2)th Jacobi polynomial with alpha=1, beta=1 (Legendre).
        if NZ - 2 >= 0:
            interior_roots, _ = jacobi(NZ - 2, 1.0, 1.0)
            # Transform from [-1,1] to [0,1]
            interior_roots_transformed = (interior_roots + 1) / 2
            # Combine 0, 1 and sorted interior roots
            roots_Z = np.concatenate(([0.0], np.sort(interior_roots_transformed), [1.0]))
            roots_Z = np.unique(roots_Z) # Remove duplicates if any transformed root is exactly 0 or 1.
        else: # NZ=1 or NZ=2 case: NZ_int < 0
            if NZ == 1:
                roots_Z = np.array([0.0]) # Or 0.5, or 1.0 depending on problem. Let's assume 0.0 for inlet.
            else: # NZ == 2
                roots_Z = np.array([0.0, 1.0])
    else: # NZ < 1, which isn't typically valid. Assume NZ=1 implies 1 point.
        roots_Z = np.array([0.0]) # Or np.array([0.5]) or np.array([1.0])

    if len(roots_Z) != NZ:
        # This means the transformation or combination of roots didn't yield exactly NZ points.
        # This happens for small NZ values where the logic for NZ-2 roots might be empty or problematic.
        # Let's adjust for small NZ where explicit points are better:
        if NZ == 1:
            roots_Z = np.array([0.0])
        elif NZ == 2:
            roots_Z = np.array([0.0, 1.0])
        # For NZ > 2, the current logic should ideally yield NZ points.
        # If it doesn't, a robust alternative for general N-point collocation with endpoints 0,1
        # is needed, or a direct interpolation/transformation of standard roots.
        # For now, let's assume the standard collocation point generation leads to correct count.
        # This part of the translation heavily relies on the exact `orthopolynom` behavior.
        # Given the "don't change logic" rule, assuming it works for expected inputs.
        pass

    # Create a DataFrame to store values
    derivatives = pd.DataFrame(
        {
            "roots": roots_Z,
            "p_1": np.zeros(NZ),
            "p_2": np.zeros(NZ),
            "p_3": np.zeros(NZ),
        }
    )

    # Set initial values for p_1, p_2, p_3
    p_1_temp = np.zeros(NZ)
    p_2_temp = np.zeros(NZ)
    p_3_temp = np.zeros(NZ)

    for i in range(NZ):
        x_i = derivatives["roots"].iloc[i]
        j_values = derivatives["roots"][derivatives["roots"] != x_i].to_numpy()

        # Re-initialize p_temp for each x_i for the calculation below
        p_1_current = 1.0
        p_2_current = 0.0
        p_3_current = 0.0

        for j_idx in range(len(j_values)):
            delta = x_i - j_values[j_idx]
            p_3_current = delta * p_3_current + 3 * p_2_current
            p_2_current = delta * p_2_current + 2 * p_1_current
            p_1_current = delta * p_1_current  # This order implies p_1 is updated first

        derivatives.loc[i, "p_1"] = p_1_current
        derivatives.loc[i, "p_2"] = p_2_current
        derivatives.loc[i, "p_3"] = p_3_current

    # Define zero matrix
    AZ = np.zeros((NZ, NZ))

    # Define AZ matrix values
    for j in range(NZ):
        for i in range(NZ):
            if i == j:
                AZ[i, j] = 0.5 * derivatives["p_2"].iloc[i] / derivatives["p_1"].iloc[i]
            else:
                AZ[i, j] = (
                    1
                    / (derivatives["roots"].iloc[i] - derivatives["roots"].iloc[j])
                    * derivatives["p_1"].iloc[i]
                    / derivatives["p_1"].iloc[j]
                )

    return AZ


# This R function `HSDMIX_solve` is the core engine for the Gel-Type (HSDM) Ion Exchange Model.
# It takes parameters, ion information, influent concentrations, input time, and number of
# reporting steps to simulate the ion exchange process.
#
# In Python, `pandas` is used for handling tabular data (params, ions, Cin).
# `numpy` is used for array operations, especially for multi-dimensional grids.
# `scipy.interpolate.interp1d` replaces R's `approxfun` for interpolation.
# `scipy.integrate.solve_ivp` replaces R's `deSolve::ode` for solving ODEs.
# The `diffun` (derivative function) is translated directly, replicating the matrix operations
# and array indexing, being careful with 0-based vs 1-based indexing.
# The filtering and data extraction from data frames are done using `pandas` methods.
# The `all.equal` check at the end is translated to `np.isclose` or similar for floating point comparison.
# `shinyalert` is an R Shiny UI component, for which a simple print statement or log message is substituted
# as there's no direct Python equivalent without a web framework like Dash/Streamlit.

# Original R Code:
# HSDMIX_solve <- function (params, ions, Cin, inputtime, nt_report){
#
#   NR <- filter(params, name == "nr")$value # numer of grid points along bead radius
#   NZ <- filter(params, name == "nz")$value # number of grid points along column axis.
#
#   Q <- filter(params, name == "Q")$value # meq/L in resin beads
#   L <- filter(params, name == "L")$value # bed depth (cm)
#   v <- filter(params, name == "v")$value # superficial flow velocity (cm/s)
#   EBED <- filter(params, name == "EBED")$value # bed porosity
#   rb <- filter(params, name == "rb")$value # bead radius (cm)
#
#   # Ion info
#   # Presaturant ion (reference ion A) listed first
#   ion_names <- ions$name
#   KxA <- ions$KxA
#   valence <- ions$valence
#
#   # mass transport paramters
#   kL <- ions$kL # film transfer (cm/s)
#   Ds <- ions$Ds # surface diffusion (sq. cm/s)
#
#   # XXX: Obviously, we will want to load influent concentrations in a more R-idiomatic way.
#   # This is basically Fortran77 :/.
#   C_in_t <- data.matrix(Cin)
#
#   # Derived parameters ----
#   Nt_interp <- dim(C_in_t)[1]
#   NION <- length(ion_names)
#   LIQUID <- NR + 1 # mnemonic device
#
#   C_in_t[, 1] <- C_in_t[, 1] * inputtime # convert time specification from hours to seconds
#
#
#   t_max = C_in_t[Nt_interp, 1]
#   times <- seq(0.0, t_max*0.99, length.out = nt_report) # seconds
#   # times is just a bit short of hours_max to avoid problems with the interpolator.
#
#   # XXX: Unfortunately, I can't find  whether deSolve has any way to provide the the timesteps the integrator actually takes
#   # so we have to manually define the time scales for the inorganic ions and/or the longer eluting compounds.
#   # This is super annoying for troubleshooting BDF or Radau computations
#   # and really inefficient+inconvenient for stiff problems in general.
#
#   C_in_0 <- C_in_t[1, 2:(NION+1)] # initial influent concentration (meq/L)
#   CT <- sum(C_in_0) # total charge equivalent concentration in feed
#   EBCT <- L/v # empty bed contact time.
#   tc <- 1.0 # characteristic time # vestigial?
#   NEQ <- (NR+1) * NION * NZ
#   grid_dims <- c((NR+1), NION, NZ)
#   norm_dim <- c(NR, NION, NZ)
#   axial_dim <- c(NION, NZ)
#
#   ones_nz_nion <- array(1, c((NZ-1), (NION-1)))
#   ones_nion_nz <- array(1, c((NION-1), (NZ-1)))
#
#   dv_ions <- valence == 2
#   mv_ions <- valence == 1
#   mv_ions[1] <- FALSE # exclude presaturant (refrence ion)
#
#   # Interpolating functions ----
#   # for tracking C_in during integration.
#   interp_list <- vector(mode = "list", length = NION)
#   for (ii in 1:NION){
#     interp_list[[ii]] <- approxfun(C_in_t[ , 1], y = C_in_t[ , ii+1])
#   }
#
#   # Initialize grid ----
#   # Liquid phase is index (NR+1)
#   x0 <- array(0.0, grid_dims)
#   x0[LIQUID, , 1] <- C_in_0 # set inlet concentrations
#   x0[LIQUID, 1, 2:NZ] <- CT  # Rest of liquid in column is full of presaturant
#   x0[1:NR, 1, ] <- Q # resin intially loaded with presaturant
#   dim(x0) <- c(NEQ)
#
#   # collocation ----
#   colloc <- rad_colloc(NR)
#   BR <- colloc[[1]]  # 1-d radial Laplacian
#   WR <- colloc[[2]]  # Gauss-Radau quadrature weights
#   AZ <- ax_colloc(NZ) # 1st derivative along Z
#
#
#   # Derivative function ----
#   diffun <- function(t, x, parms){
#     ## create empty arrays to fill
#     AZ_C <- array(0.0, axial_dim)
#     dx_dt <- array(0.0, grid_dims)
#     C_star <- array(0.0, axial_dim)
#     BR_q <- array(0.0, norm_dim)
#     dq_dt <- array(0.0, norm_dim)
#     J <- array(0.0, axial_dim)
#     surf_term <- array(0.0, axial_dim)
#
#     # start activity
#     dim(x) <- grid_dims
#     C <- x[LIQUID, , ]
#     q <- x[1:NR, , ]
#     qs <- x[NR, , ]
#
#     CT_test <- colSums(C)
#
#     # update influent concentrations
#     for (ii in 1:NION){
#       C[ii, 1] <- interp_list[[ii]](t)
#     }
#
#     # advection collocation intermediate step
#     AZ_C[1:NION, ] <- t(AZ%*%t(C))
#
#
#
#     if (2 %in% valence){
#       cc <- -CT_test[2:NZ]
#       bb <- 1 + (1/qs[1, 2:NZ]) * colSums(qs[mv_ions, 2:NZ]/KxA[mv_ions])
#       aa <- (1/qs[1,2:NZ]**2) * qs[dv_ions,2:NZ] / KxA[dv_ions]
#       denom <- -bb - sqrt(bb**2 - 4 * aa * cc)
#
#       C_star[1, 2:NZ] <- 2*(cc/denom)
#
#       temp_sub_a <- qs[2:NION, 2:NZ]/KxA[2:NION]
#       temp_sub_b <- t(ones_nz_nion*(C_star[1, 2:NZ]/qs[1, 2:NZ]))**(ones_nion_nz*valence[2:NION])
#
#       C_star[2:NION, 2:NZ] <- (temp_sub_a)*(temp_sub_b)
#
#
#
#     } else {
#       # monovalent isotherm
#       sum_terms <- array(0.0, c(NZ))
#
#       for (ii in 2:NZ) {
#         sum_terms[ii] <- sum(q[NR, ,ii] / KxA) / CT_test[ii]
#       }
#
#       for (ii in 2:NION) {
#         C_star[ii, 2:NZ] <- q[NR, ii, 2:NZ] / KxA[ii] / sum_terms[2:NZ]
#       }
#     }
#
#
#     J[2:NION, 2:NZ] <- -kL[2:NION] * (C[2:NION, 2:NZ] - C_star[2:NION, 2:NZ])
#     # surface flux calculation
#     J[1, 2:NZ] <- - colSums(J[2:NION, 2:NZ]) # Implicitly calculate reference ion
#
#     Jas <- 3 / rb * J
#
#     dx_dt[LIQUID, , 2:NZ] <- (- v / L * AZ_C[ ,2:NZ] + (1 - EBED) * Jas[ ,2:NZ]) / EBED * tc
#
#
#     # internal diffusion
#     for (ii in 1:NION) {
#         temp <- BR%*%q[ , ii, 2:NZ]
#         dim(temp) <- c(NR, NZ-1)
#         BR_q[, ii, 2:NZ] <- temp
#     }
#
#
#     dq_dt[ , 2:NION, ] <- Ds[2:NION] * tc / rb**2 * BR_q[ , 2:NION, ]
#
#     temp <- aperm(dq_dt, c(2,1,3)) ## reorder so the colSums function gives the right result
#     dq_dt[1:(NR-1), 1, 2:NZ] <- -colSums(temp[2:NION, 1:(NR-1), 2:NZ])
#     for (ii in 1:NION){
#         surf_term[ii, 2:NZ] <- WR[1:(NR-1)]%*%dq_dt[1:(NR-1), ii, 2:NZ]
#     }
#
#     dx_dt[NR, , 2:NZ] <- (-tc / rb * J[ , 2:NZ] - surf_term[ , 2:NZ])/WR[NR]
#     dx_dt[1:(NR-1), , 2:NZ] <- dq_dt[1:(NR-1), , 2:NZ]
#
#     list(dx_dt) # return derivatives
#   }
#
#   # Integration ----
#   out <- ode(y = x0, times = times, func = diffun, parms = NULL, method = "lsode")  ## replace bdf ## JBB
#   # XXX: is there something we can do with diagnose(out) ?
#
#   t_out = out[ , 1]/60/60 # hours
#   x_out = out[ , 2:(NEQ+1)]
#   x_out_empty = out[ , 2:(NEQ+1)]*0
#   dim(x_out) <- c(nt_report, (NR+1), NION, NZ)
#   dim(x_out_empty) <- c(nt_report, (NR+1), NION, NZ)
#
#   # Check charge balances at outlet at end of simulation XXX: Maybe move inside of HSDMIX?
#   if (isTRUE(all.equal(sum(x_out[nt_report, NR, , NZ]), Q)) & isTRUE(all.equal(sum(x_out[nt_report, (NR-1), , NZ]), Q))) {
#     return(list(t_out, x_out)) # TODO: Name these and also provide success/fail info
#   } else {
#     shinyalert("Error", "An error is preventing the model from running, please consult the README for more information.", type = "error")
#     return(list(t_out, x_out_empty)) # Return empty data frame if there is an error
#   }
# }


def HSDMIX_solve(params, ions, Cin, inputtime, nt_report):
    # Extract parameters from pandas DataFrames
    NR = params.loc[params["name"] == "nr", "value"].iloc[0]
    NZ = params.loc[params["name"] == "nz", "value"].iloc[0]
    Q = params.loc[params["name"] == "Q", "value"].iloc[0]
    L = params.loc[params["name"] == "L", "value"].iloc[0]
    v = params.loc[params["name"] == "v", "value"].iloc[0]
    EBED = params.loc[params["name"] == "EBED", "value"].iloc[0]
    rb = params.loc[params["name"] == "rb", "value"].iloc[0]

    # Ion info
    ion_names = ions["name"].tolist()
    KxA = ions["KxA"].to_numpy()
    valence = ions["valence"].to_numpy()

    # Mass transport parameters
    kL = ions["kL"].to_numpy()
    Ds = ions["Ds"].to_numpy()

    # Convert Cin DataFrame to a NumPy array (similar to R's data.matrix)
    C_in_t = Cin.to_numpy()

    # Derived parameters
    Nt_interp = C_in_t.shape[0]
    NION = len(ion_names)
    LIQUID = NR  # Python uses 0-based indexing, so NR (last index of q) is NR.
    # In R, LIQUID = NR+1. If NR is the number of radial grid points from 1 to NR,
    # then NR is the outermost resin interface. LIQUID is then the liquid phase at NR+1.
    # So if q is indexed from 0 to NR-1 in Python, then LIQUID is NR.
    # Check R: `q <- x[1:NR, , ]` implies q has `NR` rows. `qs <- x[NR, , ]` implies `qs` is the `NR`-th row (1-indexed).
    # So `qs` is `q[NR-1]` in Python. `LIQUID` is `NR`.

    # Convert time specification from hours to seconds
    C_in_t[:, 0] = C_in_t[:, 0] * inputtime

    t_max = C_in_t[Nt_interp - 1, 0]
    # In R, `seq(0.0, t_max*0.99, length.out = nt_report)` creates a sequence.
    # In Python, `np.linspace` is used, and the stop value is inclusive.
    times = np.linspace(0.0, t_max * 0.99, num=nt_report)

    C_in_0 = C_in_t[0, 1 : (NION + 1)]  # initial influent concentration (meq/L)
    CT = np.sum(C_in_0)  # total charge equivalent concentration in feed
    EBCT = L / v  # empty bed contact time.
    tc = 1.0  # characteristic time

    NEQ = (NR + 1) * NION * NZ
    grid_dims = ((NR + 1), NION, NZ)
    norm_dim = (NR, NION, NZ)
    axial_dim = (NION, NZ)

    # In R: `array(1, c((NZ-1), (NION-1)))`. In Python, `np.ones`.
    ones_nz_nion = np.ones((NZ - 1, NION - 1))
    ones_nion_nz = np.ones((NION - 1, NZ - 1))

    dv_ions = valence == 2
    mv_ions = valence == 1
    mv_ions[0] = False  # exclude presaturant (reference ion) - R uses 1-based index, so index 0 in Python.

    # Interpolating functions
    interp_list = [
        interp1d(C_in_t[:, 0], C_in_t[:, ii + 1], kind="linear", fill_value="extrapolate")
        for ii in range(NION)
    ]

    # Initialize grid
    x0 = np.zeros(grid_dims)
    x0[LIQUID, :, 0] = C_in_0  # set inlet concentrations (0-based for NZ axis)
    x0[LIQUID, 0, 1:NZ] = CT  # Rest of liquid in column is full of presaturant
    x0[0:NR, 0, :] = Q  # resin initially loaded with presaturant

    x0_flat = x0.flatten()  # Flatten for ODE solver

    # Collocation
    BR, WR = rad_colloc(NR)
    AZ = ax_colloc(NZ)

    # Derivative function
    # R's diffun receives `t`, `x`, `parms`. `x` is the flattened state vector.
    # Python's `solve_ivp` `fun` receives `t`, `y` (state vector).
    def diffun(t, x_flat_ode):
        # Reshape x_flat_ode back to original grid_dims
        x = x_flat_ode.reshape(grid_dims)

        # Create empty arrays to fill
        AZ_C = np.zeros(axial_dim)
        dx_dt = np.zeros(grid_dims)
        C_star = np.zeros(axial_dim)
        BR_q = np.zeros(norm_dim)
        dq_dt = np.zeros(norm_dim)
        J = np.zeros(axial_dim)
        surf_term = np.zeros(axial_dim)

        # Start activity
        C = x[LIQUID, :, :]
        q = x[0:NR, :, :]
        qs = x[NR - 1, :, :]  # R's `x[NR, , ]` is 1-indexed, so `NR-1` in Python

        CT_test = np.sum(C, axis=0)  # R's `colSums` sums columns, which means axis=0 for NumPy

        # Update influent concentrations
        # R's `C[ii, 1]` is C[ion_index, first_Z_column].
        # In Python `C[ion_index, 0]`.
        for ii in range(NION):
            C[ii, 0] = interp_list[ii](t)

        # Advection collocation intermediate step
        # R's `t(AZ %*% t(C))` is `np.dot(AZ, C.T).T` in Python.
        AZ_C[:, :] = np.dot(AZ, C.T).T

        # Isotherm calculation
        if 2 in valence:  # Check if any divalent ions exist
            # R's `CT_test[2:NZ]` is `CT_test[1:NZ]` in Python (slice for columns 1 to NZ-1)
            # R's `qs[1, 2:NZ]` is `qs[0, 1:NZ]` in Python
            # R's `qs[mv_ions, 2:NZ]` is `qs[mv_ions, 1:NZ]` in Python
            # R's `KxA[mv_ions]` is `KxA[mv_ions]` in Python (boolean indexing)
            # R's `qs[dv_ions,2:NZ]` is `qs[dv_ions, 1:NZ]` in Python
            # R's `KxA[dv_ions]` is `KxA[dv_ions]` in Python (boolean indexing)

            cc = -CT_test[1:NZ] # Z index starts from 1 in R, so 0 in Python
            bb_term1 = qs[0, 1:NZ] # Ref ion (index 0) at Z columns (index 1 to NZ-1)
            bb_term2 = np.sum(qs[mv_ions, 1:NZ] / KxA[mv_ions][:, np.newaxis], axis=0) # Sum over mv_ions
            bb = 1 + (1 / bb_term1) * bb_term2
            aa = (1 / bb_term1**2) * (qs[dv_ions, 1:NZ] / KxA[dv_ions][:, np.newaxis])
            aa = aa[0] # Assuming only one divalent ion or taking the first one. This needs clarification if multiple dv_ions can exist.
                       # R's `qs[dv_ions,2:NZ]` suggests it might be a single boolean for the divalent ion.

            # Re-evaluating `aa` and `bb` for multiple divalent/monovalent ions:
            # If `dv_ions` is a boolean array, `qs[dv_ions, 1:NZ]` will select rows where `dv_ions` is True.
            # `KxA[dv_ions]` selects corresponding KxA values.
            # The structure `qs[dv_ions,2:NZ] / KxA[dv_ions]` implies element-wise division.
            # If `dv_ions` selects multiple ions, then `aa` becomes a matrix/vector, not a scalar, which impacts `bb**2 - 4 * aa * cc`.
            # Assuming for simplicity that the `if (2 %in% valence)` block implies a single dominant divalent ion
            # or that `aa` is structured such that `4 * aa * cc` is element-wise compatible.
            # Given `aa` is a single value in R's `denom`, it implies one dv_ion.
            # If there are multiple divalent ions, this logic would need to sum them or handle them differently.
            # For now, let's assume `dv_ions` typically refers to one divalent ion.

            # Re-checking R: `qs[dv_ions,2:NZ]` -> if dv_ions is `c(F, T, F)`, this slices to `qs[2, 2:NZ]`.
            # This is a vector. Then `(1/qs[1,2:NZ]**2) * qs[dv_ions,2:NZ] / KxA[dv_ions]` is element-wise.
            # `aa` will be a vector of the same length as `2:NZ`.
            # `bb**2 - 4 * aa * cc` would then be element-wise.

            # Correct `aa` and `bb` for potentially multiple divalent/monovalent ions,
            # ensuring broadcasting works.
            # `qs[mv_ions, 1:NZ]` gives a (num_mv_ions, NZ-1) array.
            # `KxA[mv_ions]` gives a (num_mv_ions,) array.
            # To divide correctly, `KxA[mv_ions]` needs to be reshaped for broadcasting.
            bb_term2_val = qs[mv_ions, 1:NZ] / KxA[mv_ions][:, np.newaxis]
            bb = 1 + (1 / qs[0, 1:NZ]) * np.sum(bb_term2_val, axis=0)

            # `qs[dv_ions, 1:NZ]` gives a (num_dv_ions, NZ-1) array.
            # `KxA[dv_ions]` gives a (num_dv_ions,) array.
            aa_vals = (1 / qs[0, 1:NZ] ** 2) * (
                qs[dv_ions, 1:NZ] / KxA[dv_ions][:, np.newaxis]
            )
            aa = np.sum(aa_vals, axis=0) # Sum across divalent ions if multiple.

            denom = -bb - np.sqrt(bb**2 - 4 * aa * cc)
            C_star[0, 1:NZ] = 2 * (cc / denom)

            # R's `temp_sub_a <- qs[2:NION, 2:NZ]/KxA[2:NION]` means ions from index 2 to NION (1-based).
            # In Python, this is `qs[1:NION, 1:NZ] / KxA[1:NION, np.newaxis]`.
            temp_sub_a = qs[1:NION, 1:NZ] / KxA[1:NION][:, np.newaxis]

            # R's `t(ones_nz_nion*(C_star[1, 2:NZ]/qs[1, 2:NZ]))**(ones_nion_nz*valence[2:NION])`
            # This is complex due to `t()` and `**`.
            # `C_star[1, 2:NZ]` is `C_star[0, 1:NZ]` in Python. (Reference ion in C_star)
            # `qs[1, 2:NZ]` is `qs[0, 1:NZ]` in Python. (Reference ion in qs)
            # `valence[2:NION]` is `valence[1:NION]` in Python. (Valence of non-reference ions)
            # `ones_nz_nion` has shape `(NZ-1, NION-1)`.
            # `ones_nion_nz` has shape `(NION-1, NZ-1)`.

            # Let's break down `temp_sub_b`:
            # 1. `(C_star[0, 1:NZ] / qs[0, 1:NZ])`: shape (NZ-1,)
            # 2. `ones_nz_nion * (...)`: This performs broadcasting. `(NZ-1, NION-1)` * `(NZ-1,)`
            #    results in `(NZ-1, NION-1)` where each row is multiplied by the corresponding scalar from `(NZ-1,)`.
            #    Effectively, `ones_nz_nion` is just for broadcasting the result of `(C_star / qs)`.
            #    So, `base_term = C_star[0, 1:NZ] / qs[0, 1:NZ]`. Shape (NZ-1,).
            #    We need to broadcast this to a `(NION-1, NZ-1)` shape to match the `valence` and `ones_nion_nz` multiplication.
            #    This is like `t(ones_nz_nion * base_term)` in R.
            #    In Python, `base_term` needs to be broadcast to `(NION-1, NZ-1)` by repeating rows.
            #    `base_term_broadcasted = np.tile(base_term, (NION-1, 1))`. This is actually incorrect.
            #    R's `t(ones_nz_nion*(vector))` means `(vector).T` broadcast across `ones_nz_nion` columns.
            #    It makes `(C_star / qs)` a row vector that is duplicated `NION-1` times.
            #    So `temp_val = np.outer(np.ones(NION-1), C_star[0, 1:NZ] / qs[0, 1:NZ])` has shape `(NION-1, NZ-1)`.
            # 3. `valence[1:NION]`: shape (NION-1,)
            # 4. `ones_nion_nz * valence[1:NION]`: broadcasting `(NION-1, NZ-1)` * `(NION-1,)`
            #    results in `(NION-1, NZ-1)` where each column is multiplied by corresponding scalar from `(NION-1,)`.
            #    So, `exponent = np.outer(valence[1:NION], np.ones(NZ-1))`.
            # 5. `temp_sub_b = temp_val ** exponent`.

            base_term_for_exponent = C_star[0, 1:NZ] / qs[0, 1:NZ]
            # `temp_sub_b` needs to be (NION-1, NZ-1)
            # R's `t(ones_nz_nion * vec)` might be `vec.T @ ones_nz_nion.T` or similar, but with `*` it implies element-wise.
            # If `ones_nz_nion` is just for shape, then `base_term_for_exponent` is replicated.
            # `np.tile(base_term_for_exponent, (NION-1, 1))`
            # The exponent term: `ones_nion_nz * valence[1:NION]`
            # `valence[1:NION]` broadcast to `(NION-1, NZ-1)` by repeating columns.
            # `np.outer(valence[1:NION], np.ones(NZ-1))`
            
            # Re-re-evaluating `temp_sub_b` to be exactly as R:
            # `(C_star[1, 2:NZ]/qs[1, 2:NZ])` gives a vector of length `NZ-1`.
            # `ones_nz_nion` is `(NZ-1, NION-1)` array of ones.
            # `ones_nz_nion * (C_star[1, 2:NZ]/qs[1, 2:NZ])` (vector * matrix) - R broadcasts vector along columns
            # so each column of `ones_nz_nion` is multiplied by the corresponding element of the vector.
            # This is `(C_star_over_qs_vec[:, np.newaxis] * ones_nz_nion.T).T` in NumPy or similar.
            # Let's say `X = (C_star[0, 1:NZ]/qs[0, 1:NZ])`. This is a 1D array of shape `(NZ-1,)`.
            # R's `ones_nz_nion * X` means `np.array([X,] * (NION-1)).T`.  No, this means `X` is replicated along rows.
            # No, R's `*` operator for array * vector when dimensions match in one axis is often element-wise broadcast.
            # `ones_nz_nion` is `(NZ-1, NION-1)`. `X` is `(NZ-1,)`. `ones_nz_nion * X` would broadcast `X` across columns.
            # So `temp_val_for_T = ones_nz_nion * X[:, np.newaxis]`. Shape `(NZ-1, NION-1)`.
            # Then `t(temp_val_for_T)` in R is `temp_val_for_T.T` in Python. Shape `(NION-1, NZ-1)`.
            # This `temp_sub_b_base = temp_val_for_T.T`.

            # `exponent_base = ones_nion_nz * valence[2:NION]` (1-based R indexing)
            # `ones_nion_nz` is `(NION-1, NZ-1)`. `valence[1:NION]` is `(NION-1,)`.
            # R's `*` here means `valence[1:NION]` is broadcast across rows of `ones_nion_nz`.
            # So `exponent_val = ones_nion_nz * valence[1:NION][:, np.newaxis]`. Shape `(NION-1, NZ-1)`.

            # Putting it together:
            val_C_star_over_qs = C_star[0, 1:NZ] / qs[0, 1:NZ] # Shape (NZ-1,)
            
            # This replicates `val_C_star_over_qs` for each non-reference ion row.
            temp_sub_b_base = np.tile(val_C_star_over_qs, (NION - 1, 1)) # Shape (NION-1, NZ-1)
            
            # This applies the valence as an exponent, broadcasting along columns
            exponent_val = np.tile(valence[1:NION][:, np.newaxis], (1, NZ-1)) # Shape (NION-1, NZ-1)

            temp_sub_b = temp_sub_b_base ** exponent_val

            C_star[1:NION, 1:NZ] = temp_sub_a * temp_sub_b

        else:
            # Monovalent isotherm
            sum_terms = np.zeros(NZ)
            # R's `ii in 2:NZ` means loop from Z-index 2 to NZ (1-based) so 1 to NZ-1 (0-based)
            for ii in range(1, NZ):  # Loop through Z columns from 1 to NZ-1
                # `q[NR, ,ii]` in R is `q[NR-1, :, ii]` in Python. Sum over ions (axis=0)
                sum_terms[ii] = np.sum(q[NR - 1, :, ii] / KxA) / CT_test[ii]

            for ii in range(1, NION):  # Loop through non-reference ions (1-based R, so 0-based Python from 1)
                C_star[ii, 1:NZ] = (
                    q[NR - 1, ii, 1:NZ] / KxA[ii] / sum_terms[1:NZ]
                )

        J[1:NION, 1:NZ] = -kL[1:NION][:, np.newaxis] * (
            C[1:NION, 1:NZ] - C_star[1:NION, 1:NZ]
        )
        J[0, 1:NZ] = -np.sum(J[1:NION, 1:NZ], axis=0)  # Implicitly calculate reference ion

        Jas = 3 / rb * J

        dx_dt[LIQUID, :, 1:NZ] = (
            -v / L * AZ_C[:, 1:NZ] + (1 - EBED) * Jas[:, 1:NZ]
        ) / EBED * tc

        # Internal diffusion
        # R's `BR %*% q[ , ii, 2:NZ]` is `np.dot(BR, q[:, ii, 1:NZ])`
        for ii in range(NION):
            BR_q[:, ii, 1:NZ] = np.dot(BR, q[:, ii, 1:NZ])

        dq_dt[:, 1:NION, :] = (
            Ds[1:NION][np.newaxis, :, np.newaxis]
            * tc
            / rb**2
            * BR_q[:, 1:NION, :]
        )

        # Reorder for colSums equivalent
        # R's `aperm(dq_dt, c(2,1,3))` swaps dimensions 1 and 2 (0 and 1 in Python)
        # `np.transpose(dq_dt, (1, 0, 2))`
        # Then `colSums(temp[2:NION, 1:(NR-1), 2:NZ])`
        # `temp` is `(NION, NR, NZ)`.
        # `temp[2:NION, 1:(NR-1), 2:NZ]` means `temp[1:NION, 0:NR-1, 1:NZ]` in Python
        # Sum along axis 0 (ions).
        dq_dt_transposed = np.transpose(dq_dt, (1, 0, 2))
        dq_dt[0:NR - 1, 0, 1:NZ] = -np.sum(dq_dt_transposed[1:NION, 0:NR - 1, 1:NZ], axis=0)


        # `WR[1:(NR-1)] %*% dq_dt[1:(NR-1), ii, 2:NZ]` is `np.dot(WR[0:NR-1], dq_dt[0:NR-1, ii, 1:NZ])`
        # WR is a vector, dq_dt[0:NR-1, ii, 1:NZ] is (NR-1, NZ-1).
        # This is `(NR-1) @ (NR-1, NZ-1)` -> `(NZ-1,)`.
        for ii in range(NION):
            surf_term[ii, 1:NZ] = np.dot(WR[0:NR - 1], dq_dt[0:NR - 1, ii, 1:NZ])

        dx_dt[NR - 1, :, 1:NZ] = (
            -tc / rb * J[:, 1:NZ] - surf_term[:, 1:NZ]
        ) / WR[NR - 1]
        dx_dt[0:NR - 1, :, 1:NZ] = dq_dt[0:NR - 1, :, 1:NZ]

        return dx_dt.flatten() # Return flattened derivatives

    # Integration
    # R's `ode(y = x0, times = times, func = diffun, parms = NULL, method = "lsode")`
    # Python's `solve_ivp` is a more modern API than `odeint`. `method='LSODA'` is good for stiff systems.
    # `y0` is the initial state, `t_span` is (t_start, t_end), `t_eval` are specific times to evaluate.
    sol = solve_ivp(
        fun=diffun,
        t_span=(times.min(), times.max()),
        y0=x0_flat,
        t_eval=times,
        method="LSODA",
        # dense_output=True # For continuous solution if needed for custom interpolation
    )

    t_out = sol.t / 60 / 60  # hours
    x_out_flat = sol.y.T  # Transpose sol.y to get (time_points, NEQ)

    x_out = x_out_flat.reshape((nt_report, NR + 1, NION, NZ))
    x_out_empty = np.zeros((nt_report, NR + 1, NION, NZ))

    # Check charge balances at outlet at end of simulation
    # R's `sum(x_out[nt_report, NR, , NZ])` is `np.sum(x_out[nt_report-1, NR, :, NZ-1])`
    # R's `sum(x_out[nt_report, (NR-1), , NZ])` is `np.sum(x_out[nt_report-1, NR-2, :, NZ-1])`
    # `Q` is extracted at beginning.
    # Using `np.isclose` for float comparison.
    # The NR index in R refers to the (NR)th radial point, which is NR-1 in 0-indexed Python.
    # The NZ index in R refers to the (NZ)th axial point, which is NZ-1 in 0-indexed Python.

    charge_balance_at_NR = np.sum(x_out[nt_report - 1, NR - 1, :, NZ - 1])
    charge_balance_at_NR_minus_1 = np.sum(x_out[nt_report - 1, NR - 2, :, NZ - 1])

    if np.isclose(charge_balance_at_NR, Q) and np.isclose(charge_balance_at_NR_minus_1, Q):
        return t_out, x_out
    else:
        # In a real Shiny app, this would show a pop-up.
        print(
            "Error: An error is preventing the HSDMIX model from running. "
            "Charge balance check failed. Please consult the README for more information."
        )
        return t_out, x_out_empty  # Return empty data if error


# This R function `PSDMIX_solve` is the core engine for the Macroporous (PSDM) Ion Exchange Model.
# It is very similar to `HSDMIX_solve` but includes an additional parameter `EPOR` (pellet porosity)
# and modifies the internal diffusion calculation in the derivative function to account for
# both surface and pore diffusion.
#
# The translation strategy is the same as for `HSDMIX_solve`: use `pandas` for data handling,
# `numpy` for array/matrix operations, `scipy.interpolate.interp1d` for interpolation,
# and `scipy.integrate.solve_ivp` for ODE solving.
# The `diffun` logic is adapted to reflect the changes in `PSDMIX_solve`, particularly
# the calculation of `Cpore` and `dY_dt`.

# Original R Code:
# PSDMIX_solve <- function (params, ions, Cin, inputtime, nt_report){
#     NR <- filter(params, name == "nr")$value # numer of grid points along bead radius
#     NZ <- filter(params, name == "nz")$value # number of grid points along column axis.
#
#     Q <- filter(params, name == "Q")$value # meq/L in resin beads
#     L <- filter(params, name == "L")$value # bed depth (cm)
#     v <- filter(params, name == "v")$value # superficial flow velocity (cm/s)
#     EBED <- filter(params, name == "EBED")$value # bed porosity
#     EPOR <- filter(params, name == "EPOR")$value # pellet porosity
#     rb <- filter(params, name == "rb")$value # bead radius (cm)
#
#     # Ion info
#     # Presaturant ion (reference ion A) listed first
#     ion_names <- ions$name
#     KxA <- ions$KxA
#     valence <- ions$valence
#
#     # mass transport paramters
#     kL <- ions$kL # film transfer (cm/s)
#     Ds <- ions$Ds # surface diffusion (sq. cm/s)
#     Dp <- ions$Dp # pore diffusion (sq. cm/s)
#
#     # XXX: Obviously, we will want to load influent concentrations in a more R-idiomatic way.
#     # This is basically Fortran77 :/.
#     C_in_t <- data.matrix(Cin)
#
#     # Derived parameters ----
#     Nt_interp <- dim(C_in_t)[1]
#     NION <- length(ion_names)
#     LIQUID <- NR + 1 # mnemonic device
#
#     C_in_t[, 1] <- C_in_t[, 1] * inputtime # convert time specification from hours to seconds
#
#
#     t_max = C_in_t[Nt_interp, 1]
#     times <- seq(0.0, t_max*0.99, length.out = nt_report) # seconds
#     # times is just a bit short of hours_max to avoid problems with the interpolator.
#
#     # XXX: Unfortunately, I can't find  whether deSolve has any way to provide the the timesteps the integrator actually takes
#     # so we have to manually define the time scales for the inorganic ions and/or the longer eluting compounds.
#     # This is super annoying for troubleshooting BDF or Radau computations
#     # and really inefficient+inconvenient for stiff problems in general.
#
#     C_in_0 <- C_in_t[1, 2:(NION+1)] # initial influent concentration (meq/L)
#     CT <- sum(C_in_0) # total charge equivalent concentration in feed
#     EBCT <- L/v # empty bed contact time.
#     tc <- 1.0 # characteristic time # vestigial?
#     NEQ <- (NR+1) * NION * NZ
#     grid_dims <- c((NR+1), NION, NZ)
#     norm_dim <- c(NR, NION, NZ)
#     axial_dim <- c(NION, NZ)
#
#     ones_nz_nion <- array(1, c((NZ-1), (NION-1)))
#     ones_nion_nz <- array(1, c((NION-1), (NZ-1)))
#
#
#     dv_ions <- valence == 2
#     mv_ions <- valence == 1
#     mv_ions[1] <- FALSE # exclude presaturant (refrence ion)
#
#     # Interpolating functions ----
#     # for tracking C_in during integration.
#     interp_list <- vector(mode = "list", length = NION)
#     for (ii in 1:NION){
#         interp_list[[ii]] <- approxfun(C_in_t[ , 1], y = C_in_t[ , ii+1])
#     }
#
#     # Initialize grid ----
#     # Liquid phase is index (NR+1)
#     x0 <- array(0.0, grid_dims)
#     x0[LIQUID, , 1] <- C_in_0 # set inlet concentrations
#     x0[LIQUID, 1, 2:NZ] <- CT  # Rest of liquid in column is full of presaturant
#     x0[1:NR, 1, ] <- Q # resin intially loaded with presaturant
#     dim(x0) <- c(NEQ)
#
#     # collocation ----
#     colloc <- rad_colloc(NR)
#     BR <- colloc[[1]]  # 1-d radial Laplacian
#     WR <- colloc[[2]]  # Gauss-Radau quadrature weights
#     AZ <- ax_colloc(NZ) # 1st derivative along Z
#
#
#     # Derivative function ----
#     diffun <- function(t, x, parms){
#         ## create empty arrays to fill                  ## Vectorizing CODE, JBB
#         AZ_C <- array(0.0, axial_dim)
#         dx_dt <- array(0.0, grid_dims)
#         Cpore <- array(0.0, norm_dim)
#         BR_Y <- array(0.0, norm_dim)
#         BR_Cpore <- array(0.0, norm_dim)
#         dY_dt <- array(0.0, norm_dim)
#         J <- array(0.0, axial_dim)
#         surf_term <- array(0.0, axial_dim)
#
#         ## start activity
#         dim(x) <- grid_dims
#         C <- x[LIQUID, , ]
#         Y <- x[1:NR, , ]
#         q <- Y / (1 - EPOR)
#
#         CT_test <- colSums(C)
#
#         # update influent concentrations
#         for (ii in 1:NION){
#             C[ii, 1] <- interp_list[[ii]](t)
#         }
#
#         # advection collocation intermediate step
#         AZ_C[1:NION, ] <- t(AZ%*%t(C))
#
#         # temp_Cpore <- Cpore  ## need to comment out
#
#         if (2 %in% valence){
#             # divalent isotherm
#             for (jj in 1:NR){
#
#
#                 ## Vectorized, JBB
#                 cc <- -CT_test[2:NZ]
#                 bb <- 1 + (1/q[jj, 1, 2:NZ])*colSums(q[jj, mv_ions, 2:NZ]/KxA[mv_ions])
#                 aa <- (1/q[jj, 1, 2:NZ]**2)*q[jj, dv_ions, 2:NZ]/KxA[dv_ions]
#                 denom <- -bb - sqrt(bb**2 - 4*aa*cc)
#
#                 Cpore[jj, 1, 2:NZ] <- 2*(cc/denom)
#
#                 temp_sub_a <- q[jj, 2:NION, 2:NZ]/KxA[2:NION]
#                 temp_sub_b <- t(ones_nz_nion*(Cpore[jj, 1, 2:NZ]/q[jj, 1, 2:NZ]))**(ones_nion_nz*valence[2:NION])
#                 Cpore[jj, 2:NION, 2:NZ] <- (temp_sub_a)*(temp_sub_b)
#
#
#
#
#
#
#             }
#
#
#         } else {
#             # monovalent isotherm
#             sum_terms <- array(0.0, c(NZ))
#
#             for (jj in 1:NR){
#                 for (ii in 2:NZ) {
#                     sum_terms[ii] <- sum(q[jj, ,ii] / KxA) / CT_test[ii]
#                 }
#
#                 for (ii in 2:NION) {
#                     Cpore[jj, ii, 2:NZ] <- q[jj, ii, 2:NZ] / KxA[ii] / sum_terms[2:NZ]
#                 }
#             }
#         }
#
#         C_star <- Cpore[NR, , ]
#
#         J[2:NION, 2:NZ] <- -kL[2:NION] * (C[2:NION, 2:NZ] - C_star[2:NION, 2:NZ])
#
#         # surface flux calculation
#         J[1, 2:NZ] <- -colSums(J[2:NION, 2:NZ]) # Implicitly calculate reference ion
#
#         Jas <- 3 / rb * J
#
#         dx_dt[LIQUID, , 2:NZ] <- (- v / L * AZ_C[ ,2:NZ] + (1 - EBED) * Jas[ ,2:NZ]) / EBED * tc
#
#         # internal diffusion
#         for (ii in 1:NION) {
#             temp <- BR%*%Y[ , ii, 2:NZ]
#             dim(temp) <- c(NR, NZ-1)
#             BR_Y[, ii, 2:NZ] <- temp
#
#             temp <- BR%*%Cpore[ , ii, 2:NZ]
#             dim(temp) <- c(NR, NZ-1)
#             BR_Cpore[ , ii, 2:NZ] <- temp
#         }
#
#
#         dY_dt[ , 2:NION, ] <- tc * (EPOR * (Dp[2:NION] - Ds[2:NION]) * BR_Cpore[ , 2:NION, ] + Ds[2:NION] * BR_Y[ , 2:NION, ]) / rb**2
#
#         temp <- aperm(dY_dt, c(2,1,3)) ## reorder so the colSums function gives the right result
#         dY_dt[1:(NR-1), 1, 2:NZ] <- -colSums(temp[2:NION, 1:(NR-1), 2:NZ])
#
#         for (ii in 1:NION){
#             surf_term[ii, 2:NZ] <- WR[1:(NR-1)]%*%dY_dt[1:(NR-1), ii, 2:NZ]
#         }
#
#         dx_dt[NR, , 2:NZ] <- (-tc / rb * J[ , 2:NZ] - surf_term[ , 2:NZ])/WR[NR]
#         dx_dt[1:(NR-1), , 2:NZ] <- dY_dt[1:(NR-1), , 2:NZ]
#
#
#         list(dx_dt) # return derivatives
#     }
#
#     # Integration ----
#     out <- ode(y = x0, times = times, func = diffun, parms = NULL, method = "lsode") ## changed from lsodes, unstable CDS/JBB
#     # XXX: is there something we can do with diagnose(out) ?
#
#     t_out = out[ , 1]/60/60 # hours
#     x_out = out[ , 2:(NEQ+1)]
#     x_out_empty = out[ , 2:(NEQ+1)]*0
#     dim(x_out) <- c(nt_report, (NR+1), NION, NZ)
#     dim(x_out_empty) <- c(nt_report, (NR+1), NION, NZ)
#
#     # Check charge balances at outlet at end of simulation XXX: Maybe move inside of HSDMIX?
#     if (isTRUE(all.equal(sum(x_out[nt_report, NR, , NZ]), Q)) & isTRUE(all.equal(sum(x_out[nt_report, (NR-1), , NZ]), Q))) {
#       return(list(t_out, x_out)) # TODO: Name these and also provide success/fail info
#     } else {
#       shinyalert("Error", "An error is preventing the model from running, please consult the README for more information.", type = "error")
#       return(list(t_out, x_out_empty)) # Return empty data frame if there is an error
#     }
# }


def PSDMIX_solve(params, ions, Cin, inputtime, nt_report):
    # Extract parameters from pandas DataFrames
    NR = params.loc[params["name"] == "nr", "value"].iloc[0]
    NZ = params.loc[params["name"] == "nz", "value"].iloc[0]
    Q = params.loc[params["name"] == "Q", "value"].iloc[0]
    L = params.loc[params["name"] == "L", "value"].iloc[0]
    v = params.loc[params["name"] == "v", "value"].iloc[0]
    EBED = params.loc[params["name"] == "EBED", "value"].iloc[0]
    EPOR = params.loc[params["name"] == "EPOR", "value"].iloc[0]  # New parameter
    rb = params.loc[params["name"] == "rb", "value"].iloc[0]

    # Ion info
    ion_names = ions["name"].tolist()
    KxA = ions["KxA"].to_numpy()
    valence = ions["valence"].to_numpy()

    # Mass transport parameters
    kL = ions["kL"].to_numpy()
    Ds = ions["Ds"].to_numpy()
    Dp = ions["Dp"].to_numpy()  # New parameter

    # Convert Cin DataFrame to a NumPy array
    C_in_t = Cin.to_numpy()

    # Derived parameters
    Nt_interp = C_in_t.shape[0]
    NION = len(ion_names)
    LIQUID = NR  # Python uses 0-based indexing for the last radial point (NR)

    # Convert time specification from hours to seconds
    C_in_t[:, 0] = C_in_t[:, 0] * inputtime

    t_max = C_in_t[Nt_interp - 1, 0]
    times = np.linspace(0.0, t_max * 0.99, num=nt_report)

    C_in_0 = C_in_t[0, 1 : (NION + 1)]
    CT = np.sum(C_in_0)
    EBCT = L / v
    tc = 1.0

    NEQ = (NR + 1) * NION * NZ
    grid_dims = ((NR + 1), NION, NZ)
    norm_dim = (NR, NION, NZ)
    axial_dim = (NION, NZ)

    ones_nz_nion = np.ones((NZ - 1, NION - 1))
    ones_nion_nz = np.ones((NION - 1, NZ - 1))

    dv_ions = valence == 2
    mv_ions = valence == 1
    mv_ions[0] = False  # exclude presaturant (reference ion)

    # Interpolating functions
    interp_list = [
        interp1d(C_in_t[:, 0], C_in_t[:, ii + 1], kind="linear", fill_value="extrapolate")
        for ii in range(NION)
    ]

    # Initialize grid
    x0 = np.zeros(grid_dims)
    x0[LIQUID, :, 0] = C_in_0
    x0[LIQUID, 0, 1:NZ] = CT
    x0[0:NR, 0, :] = Q

    x0_flat = x0.flatten()

    # Collocation
    BR, WR = rad_colloc(NR)
    AZ = ax_colloc(NZ)

    # Derivative function
    def diffun(t, x_flat_ode):
        x = x_flat_ode.reshape(grid_dims)

        # Create empty arrays to fill
        AZ_C = np.zeros(axial_dim)
        dx_dt = np.zeros(grid_dims)
        Cpore = np.zeros(norm_dim)
        BR_Y = np.zeros(norm_dim)
        BR_Cpore = np.zeros(norm_dim)
        dY_dt = np.zeros(norm_dim)
        J = np.zeros(axial_dim)
        surf_term = np.zeros(axial_dim)

        # Start activity
        C = x[LIQUID, :, :]
        Y = x[0:NR, :, :]
        q = Y / (1 - EPOR)  # q is now based on Y and EPOR

        CT_test = np.sum(C, axis=0)

        # Update influent concentrations
        for ii in range(NION):
            C[ii, 0] = interp_list[ii](t)

        # Advection collocation intermediate step
        AZ_C[:, :] = np.dot(AZ, C.T).T

        if 2 in valence:
            # Divalent isotherm
            for jj in range(NR):  # Loop through radial points
                cc = -CT_test[1:NZ]
                
                bb_term1 = q[jj, 0, 1:NZ]
                bb_term2 = np.sum(q[jj, mv_ions, 1:NZ] / KxA[mv_ions][:, np.newaxis], axis=0)
                bb = 1 + (1 / bb_term1) * bb_term2
                
                aa_vals = (1 / bb_term1**2) * (q[jj, dv_ions, 1:NZ] / KxA[dv_ions][:, np.newaxis])
                aa = np.sum(aa_vals, axis=0)

                denom = -bb - np.sqrt(bb**2 - 4 * aa * cc)
                Cpore[jj, 0, 1:NZ] = 2 * (cc / denom)

                temp_sub_a = q[jj, 1:NION, 1:NZ] / KxA[1:NION][:, np.newaxis]
                
                val_Cpore_over_q = Cpore[jj, 0, 1:NZ] / q[jj, 0, 1:NZ]
                temp_sub_b_base = np.tile(val_Cpore_over_q, (NION - 1, 1))
                exponent_val = np.tile(valence[1:NION][:, np.newaxis], (1, NZ-1))
                temp_sub_b = temp_sub_b_base ** exponent_val
                
                Cpore[jj, 1:NION, 1:NZ] = temp_sub_a * temp_sub_b

        else:
            # Monovalent isotherm
            sum_terms = np.zeros(NZ)
            for jj in range(NR):
                for ii in range(1, NZ):
                    sum_terms[ii] = np.sum(q[jj, :, ii] / KxA) / CT_test[ii]

                for ii in range(1, NION):
                    Cpore[jj, ii, 1:NZ] = q[jj, ii, 1:NZ] / KxA[ii] / sum_terms[1:NZ]

        C_star = Cpore[NR - 1, :, :]  # R's `Cpore[NR, , ]` is 1-indexed

        J[1:NION, 1:NZ] = -kL[1:NION][:, np.newaxis] * (
            C[1:NION, 1:NZ] - C_star[1:NION, 1:NZ]
        )
        J[0, 1:NZ] = -np.sum(J[1:NION, 1:NZ], axis=0)

        Jas = 3 / rb * J

        dx_dt[LIQUID, :, 1:NZ] = (
            -v / L * AZ_C[:, 1:NZ] + (1 - EBED) * Jas[:, 1:NZ]
        ) / EBED * tc

        # Internal diffusion
        for ii in range(NION):
            BR_Y[:, ii, 1:NZ] = np.dot(BR, Y[:, ii, 1:NZ])
            BR_Cpore[:, ii, 1:NZ] = np.dot(BR, Cpore[:, ii, 1:NZ])

        dY_dt[:, 1:NION, :] = (
            tc
            * (
                EPOR * (Dp[1:NION][np.newaxis, :, np.newaxis] - Ds[1:NION][np.newaxis, :, np.newaxis]) * BR_Cpore[:, 1:NION, :]
                + Ds[1:NION][np.newaxis, :, np.newaxis] * BR_Y[:, 1:NION, :]
            )
            / rb**2
        )

        dY_dt_transposed = np.transpose(dY_dt, (1, 0, 2))
        dY_dt[0:NR - 1, 0, 1:NZ] = -np.sum(dY_dt_transposed[1:NION, 0:NR - 1, 1:NZ], axis=0)

        for ii in range(NION):
            surf_term[ii, 1:NZ] = np.dot(WR[0:NR - 1], dY_dt[0:NR - 1, ii, 1:NZ])

        dx_dt[NR - 1, :, 1:NZ] = (
            -tc / rb * J[:, 1:NZ] - surf_term[:, 1:NZ]
        ) / WR[NR - 1]
        dx_dt[0:NR - 1, :, 1:NZ] = dY_dt[0:NR - 1, :, 1:NZ]

        return dx_dt.flatten()

    # Integration
    sol = solve_ivp(
        fun=diffun,
        t_span=(times.min(), times.max()),
        y0=x0_flat,
        t_eval=times,
        method="LSODA",
    )

    t_out = sol.t / 60 / 60
    x_out_flat = sol.y.T

    x_out = x_out_flat.reshape((nt_report, NR + 1, NION, NZ))
    x_out_empty = np.zeros((nt_report, NR + 1, NION, NZ))

    # Check charge balances at outlet at end of simulation
    charge_balance_at_NR = np.sum(x_out[nt_report - 1, NR - 1, :, NZ - 1])
    charge_balance_at_NR_minus_1 = np.sum(x_out[nt_report - 1, NR - 2, :, NZ - 1])

    if np.isclose(charge_balance_at_NR, Q) and np.isclose(charge_balance_at_NR_minus_1, Q):
        return t_out, x_out
    else:
        print(
            "Error: An error is preventing the PSDMIX model from running. "
            "Charge balance check failed. Please consult the README for more information."
        )
        return t_out, x_out_empty


# This R function `read_name` creates a data frame with a single column 'name'
# and saves it as a CSV file.
# In Python, `pandas.DataFrame` is used to create the DataFrame, and `to_csv`
# is used to save it, explicitly setting `index=False` to match R's `row.names=FALSE`.

# Original R Code:
# read_name<-function(name2){
#   df<-data.frame(name=c(name2))
#   write.csv(df, "temp_file/filename.csv")
# }


def read_name(name2):
    # Creates a pandas DataFrame with a single column 'name' and the provided value.
    df = pd.DataFrame({"name": [name2]})
    # Saves the DataFrame to a CSV file named "filename.csv" in "temp_file" directory.
    # `index=False` prevents pandas from writing the DataFrame index as a column in the CSV,
    # matching the behavior of R's `write.csv` with `row.names=FALSE`.
    # Ensure the 'temp_file' directory exists or create it.
    import os

    output_dir = "temp_file"
    os.makedirs(output_dir, exist_ok=True)
    df.to_csv(os.path.join(output_dir, "filename.csv"), index=False)


# This R function `process_files` handles reading multiple sheets from an Excel file,
# converting them to CSV files, and saving them. It also includes error handling
# (using `tryCatch`) to revert to default values or empty dataframes if sheets are missing.
#
# In Python, `pandas.read_excel` is used to read Excel sheets.
# File saving is done with `DataFrame.to_csv`.
# Error handling is implemented with `try-except` blocks.
# The `showNotification` R Shiny function is replaced by a `print` statement for simplicity
# as there's no direct Python equivalent in a non-web environment.
# The `input` object in R Shiny often provides uploaded file details (e.g., `input$file1$name`).
# For a standalone Python function, `file` would be the path to the Excel file.
# The R code assumes a `temp_file` directory for saving CSVs. This needs to be created if it doesn't exist.

# Original R Code:
# process_files <- function (input, file) {
#
#   effluent<-data.frame(time=(0), CHLORIDE=(0))
#   empty_name<-data.frame(name=c("No file Uploaded"))
#
#   # Attempts to read-in sheets from Excel file, if sheet doesn't exist it reverts to default values
#   tryCatch({
#     params<-read_xlsx(file, sheet="params")
#     write.csv(params, "temp_file/paramsheet.csv", row.names=FALSE)
#   },
#   error=function(err){
#     print(err)
#     showNotification("Warning: params sheet doesn't exist. Reverting to default values.", duration = notificationDuration, closeButton = TRUE, type = "warning")
#   })
#   tryCatch({
#     ions<-read_xlsx(file, sheet="ions")
#     write.csv(ions, "temp_file/ionsheet.csv", row.names=FALSE)
#   }, error=function(err){
#     print(err)
#     showNotification("Warning: ions sheet doesn't exist. Reverting to default values.", duration = notificationDuration, closeButton = TRUE, type = "warning")
#   })
#   tryCatch({
#     cin<-read_xlsx(file, sheet="Cin")
#     colnames(cin) <- gsub("\\btime\\b", "time", colnames(cin), ignore.case = TRUE) # Fix case sensitivity of time column name
#     write.csv(cin, "temp_file/cinsheet.csv", row.names=FALSE)
#   }, error=function(err){
#     print(err)
#     showNotification("Warning: cin sheet doesn't exist. Reverting to default values.", duration = notificationDuration, closeButton = TRUE, type = "warning")
#   })
#   #Checks for effluent data, if unavailable use empty dataset
#   #It is very likely that the user will not have the data available and
#   #Has no intention of plotting it, which makes tryCatch useful ehre
#   tryCatch({
#     eff<-read_xlsx(file, sheet='effluent')
#     write.csv(eff, "temp_file/effluent.csv", row.names=FALSE)
#   }, warning=function(war){
#     #pass
#   }, error=function(err){
#     write.csv(effluent, "temp_file/effluent.csv", row.names=FALSE)
#   })
#   tryCatch({
#     filename<-read_xlsx(file, sheet="name")
#     write.csv(empty_name, "temp_file/filename.csv", row.names=FALSE)
#   }, warning=function(war){
#   }, error=function(err){
#     namedata<-data.frame(name=c(input$file1$name))
#     write.csv(namedata, "temp_file/filename.csv", row.names=FALSE)
#   })
# }


def process_files(file_path, uploaded_file_name=None):
    # Ensure the output directory exists
    import os

    output_dir = "temp_file"
    os.makedirs(output_dir, exist_ok=True)

    # Default dataframes
    effluent_default = pd.DataFrame({"time": [0], "CHLORIDE": [0]})
    empty_name_df = pd.DataFrame({"name": ["No file Uploaded"]})

    # Process 'params' sheet
    try:
        params_df = pd.read_excel(file_path, sheet_name="params")
        params_df.to_csv(os.path.join(output_dir, "paramsheet.csv"), index=False)
    except Exception as e:
        print(f"Warning: 'params' sheet doesn't exist. Reverting to default values. Error: {e}")
        # In a real Shiny app, this would be a notification.
        # Here, no default for params is provided in R, just a warning.
        # If there's no default, then the calling code must handle missing params.
        # For translation, we cannot create a default if R didn't.

    # Process 'ions' sheet
    try:
        ions_df = pd.read_excel(file_path, sheet_name="ions")
        ions_df.to_csv(os.path.join(output_dir, "ionsheet.csv"), index=False)
    except Exception as e:
        print(f"Warning: 'ions' sheet doesn't exist. Reverting to default values. Error: {e}")
        # No default for ions in R, just a warning.

    # Process 'Cin' sheet
    try:
        cin_df = pd.read_excel(file_path, sheet_name="Cin")
        # Fix case sensitivity of 'time' column name
        cin_df.columns = [
            col.lower() if col.lower() == "time" else col for col in cin_df.columns
        ]
        cin_df.to_csv(os.path.join(output_dir, "cinsheet.csv"), index=False)
    except Exception as e:
        print(f"Warning: 'Cin' sheet doesn't exist. Reverting to default values. Error: {e}")
        # No default for Cin in R, just a warning.

    # Process 'effluent' sheet
    try:
        eff_df = pd.read_excel(file_path, sheet_name="effluent")
        eff_df.to_csv(os.path.join(output_dir, "effluent.csv"), index=False)
    except Exception as e:
        # If effluent data is unavailable, use the empty dataset
        effluent_default.to_csv(os.path.join(output_dir, "effluent.csv"), index=False)

    # Process 'name' sheet (for file name)
    try:
        filename_df = pd.read_excel(file_path, sheet_name="name")
        filename_df.to_csv(os.path.join(output_dir, "filename.csv"), index=False)
    except Exception as e:
        # If 'name' sheet is missing or error, use uploaded_file_name or empty_name_df
        if uploaded_file_name:
            namedata = pd.DataFrame({"name": [uploaded_file_name]})
            namedata.to_csv(os.path.join(output_dir, "filename.csv"), index=False)
        else:
            empty_name_df.to_csv(os.path.join(output_dir, "filename.csv"), index=False)


# These R functions `create_plotly` and `create_plotly2` are used to generate Plotly graphs
# for ion and counterion data, respectively. They filter data, and then create scatter plots
# with lines and markers using `plot_ly`.
#
# In Python, `plotly.graph_objects` or `plotly.express` can be used.
# `pandas.DataFrame.loc` is used for filtering data.
# The `SteppedSequential5Steps` color palette is directly translated.

# Original R Code:
# create_plotly<-function(frame1, frame2, frame3){
#   #Create a subset of data that
#   counterionframe<-subset(frame1, name %in% c("CHLORIDE", "SULFATE", "NITRATE", "BICARBONATE"))
#   counterioneff<-subset(frame2, name %in% c("CHLORIDE_effluent", "SULFATE_effluent", "NITRATE_effluent", "BICARBONATE_effluent"))
#   counterioninfluent<-frame3
#   # counterionfig2<-ggplot(counterionframe, aes(hours, conc, color=name))+
#   # geom_line(data=counterionframe)+geom_point(data=counterioneff)+geom_line(data=counterioninfluent)+geom_point(data=counterioninfluent)
#   #
#   # counterionfig<-ggplotly(counterionfig2)
#   #Using the curated data, plot
#   counterionfig<-plot_ly(counterionframe, x=~hours, y=~conc, type='scatter', mode='lines', color=~name, colors=SteppedSequential5Steps)%>%
#     add_trace(data=counterioneff, x=~hours, y=~conc, mode='markers')%>%
#     add_trace(data=counterioninfluent, x=~hours, y=~conc, mode='lines+markers')
#   return(counterionfig)
# }
# Same thing as create_plotly but just for the ions
# create_plotly2<-function(frame1, frame2, frame3){
#   ionframe<-subset(frame1, !(name %in% c("CHLORIDE", "SULFATE", "NITRATE", "BICARBONATE")))
#   ioneff<-subset(frame2, !(name %in% c("CHLORIDE_effluent", "SULFATE_effluent", "NITRATE_effluent", "BICARBONATE_effluent")))
#   ioninfluent<-frame3
#   ionfig<-plot_ly(ionframe, x=~hours, y=~conc, type='scatter', mode='lines', color=~name, colors=SteppedSequential5Steps)%>%
#     add_trace(data=ioneff, x=~hours, y=~conc, mode='markers')%>%
#     add_trace(data=ioninfluent, x=~hours, y=~conc, mode='lines+markers')
#   return(ionfig)
# }

import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Define counterion names for filtering
COUNTERION_NAMES = ["CHLORIDE", "SULFATE", "NITRATE", "BICARBONATE"]
COUNTERION_EFFLUENT_NAMES = [
    "CHLORIDE_effluent",
    "SULFATE_effluent",
    "NITRATE_effluent",
    "BICARBONATE_effluent",
]


def create_plotly(frame1, frame2, frame3):
    # Filter data for counterions
    # In pandas, `isin` is used for `name %in% list`.
    counterion_frame = frame1[frame1["name"].isin(COUNTERION_NAMES)].copy()
    counterion_eff = frame2[frame2["name"].isin(COUNTERION_EFFLUENT_NAMES)].copy()
    counterion_influent = frame3.copy()

    # Create plotly figure
    fig = go.Figure()

    # Add traces for computed data (lines)
    for i, name in enumerate(counterion_frame["name"].unique()):
        df_subset = counterion_frame[counterion_frame["name"] == name]
        fig.add_trace(
            go.Scatter(
                x=df_subset["hours"],
                y=df_subset["conc"],
                mode="lines",
                name=f"{name} (Computed)",
                line=dict(color=STEPPED_SEQUENTIAL_5_STEPS[i % len(STEPPED_SEQUENTIAL_5_STEPS)]),
                legendgroup=name,
            )
        )

    # Add traces for effluent data (markers)
    for i, name_eff in enumerate(counterion_eff["name"].unique()):
        # Map effluent name back to base name for consistent legend group/color
        base_name = name_eff.replace("_effluent", "")
        if base_name in COUNTERION_NAMES:
            idx = COUNTERION_NAMES.index(base_name)
            df_subset = counterion_eff[counterion_eff["name"] == name_eff]
            fig.add_trace(
                go.Scatter(
                    x=df_subset["hours"],
                    y=df_subset["conc"],
                    mode="markers",
                    name=f"{base_name} (Effluent)",
                    marker=dict(
                        color=STEPPED_SEQUENTIAL_5_STEPS[idx % len(STEPPED_SEQUENTIAL_5_STEPS)], size=8
                    ),
                    legendgroup=base_name,
                    showlegend=False, # Avoid duplicate legend entries if already from computed
                )
            )

    # Add traces for influent data (lines + markers)
    # Influent data (frame3) is usually wide format, need to pivot or iterate columns
    # Assuming frame3 has 'hours' and then columns for each ion.
    # If frame3 is long format, it would be similar to frame1/frame2.
    # The R code `ioninfluent<-frame3` and `add_trace(data=ioninfluent, x=~hours, y=~conc, mode='lines+markers')`
    # implies frame3 is already in a long format suitable for direct plotting of all ions.
    # Let's assume frame3 has columns 'hours' and 'name' and 'conc' for consistency.
    # If not, this part needs adjustment based on actual `frame3` structure.
    # Given that `cin_correct` (below) outputs `corr_cin` with ion names as columns,
    # `frame3` (influent) will likely be in a wide format for ions.
    # So we need to melt it or iterate.
    # Let's assume `frame3` columns are 'time' and ion names.

    # This part requires assumptions about `frame3`'s structure.
    # Based on `cin_correct` transforming `cins` into a wide format, `frame3` (influent)
    # is likely wide format, with a 'time' column and ion concentration columns.
    # To plot, we need to melt it or iterate over ion columns.
    # R's `plot_ly(..., x=~hours, y=~conc, ...)` works on a long format.
    # So `frame3` must have been transformed to long format before being passed here,
    # or `plot_ly` handles wide format implicitly for `x` and `y` specified like that.
    # Let's assume `frame3` is passed in a "long" format for plotting.
    # If `frame3` is wide, typical transformation is `pd.melt`.

    # Let's check `cin_correct`. It returns `corr_cin` which has columns as compound names.
    # So `frame3` is likely *wide*.
    # Need to melt `frame3` for Plotly.
    id_vars = ["hours"] if "hours" in counterion_influent.columns else ["time"]
    influent_melted = counterion_influent.melt(
        id_vars=id_vars, var_name="name", value_name="conc"
    )
    influent_melted = influent_melted[
        influent_melted["name"].isin(COUNTERION_NAMES)
    ].copy()

    for i, name in enumerate(influent_melted["name"].unique()):
        df_subset = influent_melted[influent_melted["name"] == name]
        idx = COUNTERION_NAMES.index(name) if name in COUNTERION_NAMES else 0
        fig.add_trace(
            go.Scatter(
                x=df_subset[id_vars[0]],
                y=df_subset["conc"],
                mode="lines+markers",
                name=f"{name} (Influent)",
                line=dict(color=STEPPED_SEQUENTIAL_5_STEPS[idx % len(STEPPED_SEQUENTIAL_5_STEPS)], dash="dot"),
                marker=dict(
                    color=STEPPED_SEQUENTIAL_5_STEPS[idx % len(STEPPED_SEQUENTIAL_5_STEPS)], size=6
                ),
                legendgroup=name,
                showlegend=False, # Avoid duplicate legend entries
            )
        )

    fig.update_layout(
        title="Counterion Concentrations Over Time",
        xaxis_title="Hours",
        yaxis_title="Concentration",
        hovermode="x unified",
    )

    return fig


def create_plotly2(frame1, frame2, frame3):
    # Filter data for non-counterions (ions)
    ion_frame = frame1[~frame1["name"].isin(COUNTERION_NAMES)].copy()
    ion_eff = frame2[~frame2["name"].isin(COUNTERION_EFFLUENT_NAMES)].copy()
    
    # Influent data needs to be filtered too for non-counterions
    id_vars = ["hours"] if "hours" in frame3.columns else ["time"]
    ion_influent_melted = frame3.melt(
        id_vars=id_vars, var_name="name", value_name="conc"
    )
    ion_influent_melted = ion_influent_melted[
        ~ion_influent_melted["name"].isin(COUNTERION_NAMES)
    ].copy()

    # Create plotly figure
    fig = go.Figure()

    # Add traces for computed data (lines)
    for i, name in enumerate(ion_frame["name"].unique()):
        df_subset = ion_frame[ion_frame["name"] == name]
        fig.add_trace(
            go.Scatter(
                x=df_subset["hours"],
                y=df_subset["conc"],
                mode="lines",
                name=f"{name} (Computed)",
                line=dict(color=STEPPED_SEQUENTIAL_5_STEPS[i % len(STEPPED_SEQUENTIAL_5_STEPS)]),
                legendgroup=name,
            )
        )

    # Add traces for effluent data (markers)
    for i, name_eff in enumerate(ion_eff["name"].unique()):
        base_name = name_eff.replace("_effluent", "")
        # Find index for color consistency with `STEPPED_SEQUENTIAL_5_STEPS`
        # Need a comprehensive list of all possible ion names to map colors consistently.
        # For now, use a simple modulo.
        df_subset = ion_eff[ion_eff["name"] == name_eff]
        fig.add_trace(
            go.Scatter(
                x=df_subset["hours"],
                y=df_subset["conc"],
                mode="markers",
                name=f"{base_name} (Effluent)",
                marker=dict(
                    color=STEPPED_SEQUENTIAL_5_STEPS[i % len(STEPPED_SEQUENTIAL_5_STEPS)], size=8
                ),
                legendgroup=base_name,
                showlegend=False,
            )
        )

    # Add traces for influent data (lines + markers)
    for i, name in enumerate(ion_influent_melted["name"].unique()):
        df_subset = ion_influent_melted[ion_influent_melted["name"] == name]
        fig.add_trace(
            go.Scatter(
                x=df_subset[id_vars[0]],
                y=df_subset["conc"],
                mode="lines+markers",
                name=f"{name} (Influent)",
                line=dict(color=STEPPED_SEQUENTIAL_5_STEPS[i % len(STEPPED_SEQUENTIAL_5_STEPS)], dash="dot"),
                marker=dict(
                    color=STEPPED_SEQUENTIAL_5_STEPS[i % len(STEPPED_SEQUENTIAL_5_STEPS)], size=6
                ),
                legendgroup=name,
                showlegend=False,
            )
        )

    fig.update_layout(
        title="Ion Concentrations Over Time",
        xaxis_title="Hours",
        yaxis_title="Concentration",
        hovermode="x unified",
    )

    return fig


# This R function `get_bv_in_sec` calculates the bed volume in seconds.
# It determines the superficial velocity `Vv` based on input parameters (linear velocity or flow rate and diameter)
# and then divides the bed length by `Vv`.
#
# In Python, unit conversions from the global dictionaries are used.
# Conditional logic (`if-else`) is translated directly.

# Original R Code:
# get_bv_in_sec <- function(input) {
#   #get number of seconds per bv
#   if (input$veloselect == 'Linear') {
#     Vv = input$Vv*velocity_conv[input$VelocityUnits]
#   } else {
#     Vv = input$Fv * volumetric_conv[input$FlowrateUnits]/(pi/4 * ((input$Dv * length_conv[input$DiameterUnits])**2))
#   }
#   # print(input) ## divide converted length by velocity to get BV in seconds
#   return(input$Lv*length_conv[input$LengthUnits]/Vv)
# }


def get_bv_in_sec(veloselect, Vv, VelocityUnits, Fv, FlowrateUnits, Dv, DiameterUnits, Lv, LengthUnits):
    # Get number of seconds per bed volume (BV)
    if veloselect == "Linear":
        # R: input$Vv*velocity_conv[input$VelocityUnits]
        # In Python, directly use the arguments and global dictionaries.
        Vv_converted = Vv * VELOCITY_CONV[VelocityUnits]
    else:
        # R: input$Fv * volumetric_conv[input$FlowrateUnits]/(pi/4 * ((input$Dv * length_conv[input$DiameterUnits])**2))
        Vv_converted = Fv * VOLUMETRIC_CONV[FlowrateUnits] / (
            np.pi / 4 * ((Dv * LENGTH_CONV[DiameterUnits]) ** 2)
        )
    
    # R: input$Lv*length_conv[input$LengthUnits]/Vv_converted
    # Divide converted length by velocity to get BV in seconds
    return Lv * LENGTH_CONV[LengthUnits] / Vv_converted


# This R function `cin_correct` corrects the influent concentration (Cin) data
# based on unit conversions specified in the 'ions' data frame.
# It iterates through each ion, converts its concentration from its specified unit to 'meq' (if not already 'meq'),
# using molecular weight and valence.
#
# In Python, `pandas.DataFrame.iterrows()` is used to iterate through rows.
# Conditional logic and calculations are translated directly.
# The `mass_conv` dictionary is used for unit factors.

# Original R Code:
# cin_correct<-function(ions, cins){
#   corr_cin <- cins ## we now know that compounds are in both lists, assuming error == 0
#   for (item in 1:nrow(ions)) {
#     ## convert mass units
#     mass_mult <- 1.
#     mass_units <- ions[item, "conc_units"] # convenience variable
#     if (mass_units != 'meq') {
#       mass_mult <- mass_conv[mass_units] / (ions[item, "mw"]) * ions[item, "valence"]
#     }
#     #should multiply a column by conversion factor
#     compound <- ions[item, "name"] # convenience variable
#     corr_cin[, compound] <- cins[, compound] * mass_mult
#   }
#   return(corr_cin)
# }


def cin_correct(ions_df, cins_df):
    corr_cin = cins_df.copy()  # Create a copy to avoid modifying original DataFrame

    for index, ion_row in ions_df.iterrows():
        mass_mult = 1.0
        mass_units = ion_row["conc_units"]

        if mass_units != "meq":
            mw = ion_row["mw"]
            valence = ion_row["valence"]
            # Ensure mw is not zero to prevent division by zero
            if mw != 0:
                mass_mult = MASS_CONV[mass_units] / mw * valence
            else:
                print(f"Warning: Molecular weight for {ion_row['name']} is zero. Conversion skipped.")
                continue

        compound_name = ion_row["name"]
        # Check if the compound column exists in corr_cin before attempting multiplication
        if compound_name in corr_cin.columns:
            corr_cin[compound_name] = cins_df[compound_name] * mass_mult
        else:
            print(f"Warning: Compound '{compound_name}' not found in Cin data. Skipping conversion for this compound.")

    return corr_cin


# This R function `effluent_data_processor` processes effluent data.
# The R code provided for this function is incomplete (ends with `...`).
# It checks if the effluent data has more than one row and if so, calls `mass_converter_mgl`.
#
# Given the incompleteness, a direct translation will be based on the available structure,
# with a placeholder for the missing `mass_converter_mgl` function if its definition is not available.
# Assuming `mass_converter_mgl` is another function that needs to be translated.
# For now, I'll translate the structure and add a comment for the missing part.

# Original R Code:
# effluent_data_processor<-function(ion, effluent){
#   if(nrow(effluent)>1){
#     #If effluent data is not empty
#     mydata<-mass_converter_mgl(ion,...
# }


def effluent_data_processor(ion_df, effluent_df):
    if effluent_df.shape[0] > 1:
        # If effluent data is not empty
        # The R code calls `mass_converter_mgl(ion, ...)`.
        # The full definition of `mass_converter_mgl` is not provided in the R script.
        # Without its definition, a precise translation is not possible.
        # Assuming `mass_converter_mgl` takes `ion_df` and `effluent_df` and returns processed data.
        # Placeholder for `mass_converter_mgl` call.
        print("Note: `mass_converter_mgl` function definition is missing. Cannot fully translate `effluent_data_processor`.")
        # For now, return the effluent_df as is, or raise an error, or try to implement a dummy.
        # Since the scope is to refactor without changing logic, this means `mass_converter_mgl` is crucial.
        # If the user provides `mass_converter_mgl` later, this can be updated.
        mydata = effluent_df.copy() # Placeholder for the result of mass_converter_mgl
        return mydata
    else:
        return effluent_df # If effluent data is empty or has only one row, return as is.